# Configuration file that uses the Splunk exporters (SAPM, SignalFX) to push
# data to Splunk products.

extensions:
  health_check:
  http_forwarder:
    egress:
      endpoint: "${SPLUNK_API_URL}"
  zpages:

receivers:
  fluentforward:
    endpoint: 0.0.0.0:8006
  hostmetrics:
    collection_interval: 1m
    scrapers:
      cpu:
      disk:
      load:
      filesystem:
      memory:
      network:
      processes:
      paging:
  jaeger:
    protocols:
      grpc:
      thrift_binary:
      thrift_compact:
      thrift_http:
  opencensus:
  otlp:
    protocols:
      grpc:
      http:
  # This section is used to collect the OpenTelemetry Collector metrics
  # Even if just a Splunk ÂµAPM customer, these metrics are included
  prometheus:
    config:
      scrape_configs:
      - job_name: 'otel-collector'
        scrape_interval: 10s
        static_configs:
        - targets: ['0.0.0.0:8888']
        metric_relabel_configs:
          - source_labels: [ __name__ ]
            regex: '.*grpc_io.*'
            action: drop
  sapm:
  signalfx:
  zipkin:

processors:
  batch:
  # Enabling the memory_limiter is strongly recommended for every pipeline.
  # Configuration is based on the amount of memory allocated to the collector.
  # The configuration below assumes 2GB of memory. In general, the ballast
  # should be set to 1/3 of the collector's memory, the limit should be 90% of
  # the collector's memory up to 2GB, and the spike should be 25% of the
  # collector's memory up to 2GB. The simplest way to specify the ballast size is
  # set the value of SPLUNK_BALLAST_SIZE_MIB env variable. This will overrides
  # the value of --mem-ballast-size-mib command line flag. If SPLUNK_BALLAST_SIZE_MIB
  # is not defined then --mem-ballast-size-mib command line flag must be manually specified.
  # For more information about memory limiter, see
  # https://github.com/open-telemetry/opentelemetry-collector/blob/master/processor/memorylimiter/README.md
  memory_limiter:
    ballast_size_mib: ${SPLUNK_BALLAST_SIZE_MIB}
    check_interval: 2s
    limit_percentage: ${SPLUNK_MEMORY_LIMIT_PERCENTAGE}
    spike_limit_percentage: ${SPLUNK_MEMORY_SPIKE_PERCENTAGE}
  resourcedetection:
    detectors: [system]
    override: true

exporters:
  # Traces
  sapm:
    access_token: "${SPLUNK_ACCESS_TOKEN}"
    endpoint: "${SPLUNK_TRACE_URL}"
  # Metrics + Events
  signalfx:
    access_token: "${SPLUNK_ACCESS_TOKEN}"
    api_url: "${SPLUNK_API_URL}"
    ingest_url: "${SPLUNK_INGEST_URL}"
  # HEC
  splunk_hec:
    token: "${SPLUNK_HEC_TOKEN}"
    endpoint: "${SPLUNK_HEC_URL}"
    source: "otel"
    sourcetype: "otel"

service:
  pipelines:
    traces:
      receivers: [jaeger, opencensus, otlp, sapm, zipkin]
      processors: [memory_limiter, batch, resourcedetection]
      exporters: [sapm, signalfx]
    metrics:
      receivers: [opencensus, otlp, signalfx, prometheus, hostmetrics]
      processors: [memory_limiter, batch, resourcedetection]
      exporters: [signalfx]
    logs:
      receivers: [signalfx, fluentforward]
      processors: [memory_limiter, batch, resourcedetection]
      exporters: [signalfx, splunk_hec]

  extensions: [health_check, http_forwarder, zpages]
