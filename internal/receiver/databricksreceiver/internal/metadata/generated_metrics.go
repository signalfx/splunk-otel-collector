// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/confmap"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
)

// MetricSettings provides common settings for a particular metric.
type MetricSettings struct {
	Enabled bool `mapstructure:"enabled"`

	enabledProvidedByUser bool
}

// IsEnabledProvidedByUser returns true if `enabled` option is explicitly set in user settings to any value.
func (ms *MetricSettings) IsEnabledProvidedByUser() bool {
	return ms.enabledProvidedByUser
}

func (ms *MetricSettings) Unmarshal(parser *confmap.Conf) error {
	if parser == nil {
		return nil
	}
	err := parser.Unmarshal(ms, confmap.WithErrorUnused())
	if err != nil {
		return err
	}
	ms.enabledProvidedByUser = parser.IsSet("enabled")
	return nil
}

// MetricsSettings provides settings for databricksreceiver metrics.
type MetricsSettings struct {
	DatabricksJobsActiveTotal                                                                                          MetricSettings `mapstructure:"databricks.jobs.active.total"`
	DatabricksJobsRunDuration                                                                                          MetricSettings `mapstructure:"databricks.jobs.run.duration"`
	DatabricksJobsScheduleStatus                                                                                       MetricSettings `mapstructure:"databricks.jobs.schedule.status"`
	DatabricksJobsTotal                                                                                                MetricSettings `mapstructure:"databricks.jobs.total"`
	DatabricksSparkBlockManagerMemoryDiskSpaceUsed                                                                     MetricSettings `mapstructure:"databricks.spark.block_manager.memory.disk_space.used"`
	DatabricksSparkBlockManagerMemoryMax                                                                               MetricSettings `mapstructure:"databricks.spark.block_manager.memory.max"`
	DatabricksSparkBlockManagerMemoryOffHeapMax                                                                        MetricSettings `mapstructure:"databricks.spark.block_manager.memory.off_heap.max"`
	DatabricksSparkBlockManagerMemoryOffHeapUsed                                                                       MetricSettings `mapstructure:"databricks.spark.block_manager.memory.off_heap.used"`
	DatabricksSparkBlockManagerMemoryOnHeapMax                                                                         MetricSettings `mapstructure:"databricks.spark.block_manager.memory.on_heap.max"`
	DatabricksSparkBlockManagerMemoryOnHeapUsed                                                                        MetricSettings `mapstructure:"databricks.spark.block_manager.memory.on_heap.used"`
	DatabricksSparkBlockManagerMemoryRemaining                                                                         MetricSettings `mapstructure:"databricks.spark.block_manager.memory.remaining"`
	DatabricksSparkBlockManagerMemoryRemainingOffHeap                                                                  MetricSettings `mapstructure:"databricks.spark.block_manager.memory.remaining.off_heap"`
	DatabricksSparkBlockManagerMemoryRemainingOnHeap                                                                   MetricSettings `mapstructure:"databricks.spark.block_manager.memory.remaining.on_heap"`
	DatabricksSparkBlockManagerMemoryUsed                                                                              MetricSettings `mapstructure:"databricks.spark.block_manager.memory.used"`
	DatabricksSparkCodeGeneratorCompilationTime                                                                        MetricSettings `mapstructure:"databricks.spark.code_generator.compilation.time"`
	DatabricksSparkCodeGeneratorGeneratedClassSize                                                                     MetricSettings `mapstructure:"databricks.spark.code_generator.generated_class_size"`
	DatabricksSparkCodeGeneratorGeneratedMethodSize                                                                    MetricSettings `mapstructure:"databricks.spark.code_generator.generated_method_size"`
	DatabricksSparkCodeGeneratorSourcecodeSize                                                                         MetricSettings `mapstructure:"databricks.spark.code_generator.sourcecode_size"`
	DatabricksSparkDagSchedulerJobsActive                                                                              MetricSettings `mapstructure:"databricks.spark.dag_scheduler.jobs.active"`
	DatabricksSparkDagSchedulerJobsAll                                                                                 MetricSettings `mapstructure:"databricks.spark.dag_scheduler.jobs.all"`
	DatabricksSparkDagSchedulerStagesFailed                                                                            MetricSettings `mapstructure:"databricks.spark.dag_scheduler.stages.failed"`
	DatabricksSparkDagSchedulerStagesRunning                                                                           MetricSettings `mapstructure:"databricks.spark.dag_scheduler.stages.running"`
	DatabricksSparkDagSchedulerStagesWaiting                                                                           MetricSettings `mapstructure:"databricks.spark.dag_scheduler.stages.waiting"`
	DatabricksSparkDatabricksDirectoryCommitAutoVacuumCount                                                            MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.auto_vacuum.count"`
	DatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered                                                       MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.deleted_files_filtered"`
	DatabricksSparkDatabricksDirectoryCommitFilterListingCount                                                         MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.filter_listing.count"`
	DatabricksSparkDatabricksDirectoryCommitJobCommitCompleted                                                         MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.job_commit_completed"`
	DatabricksSparkDatabricksDirectoryCommitMarkerReadErrors                                                           MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.marker_read.errors"`
	DatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount                                                         MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.marker_refresh.count"`
	DatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors                                                        MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.marker_refresh.errors"`
	DatabricksSparkDatabricksDirectoryCommitMarkersRead                                                                MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.markers.read"`
	DatabricksSparkDatabricksDirectoryCommitRepeatedListCount                                                          MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.repeated_list.count"`
	DatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered                                                   MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.uncommitted_files.filtered"`
	DatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound                                                        MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.untracked_files.found"`
	DatabricksSparkDatabricksDirectoryCommitVacuumCount                                                                MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.vacuum.count"`
	DatabricksSparkDatabricksDirectoryCommitVacuumErrors                                                               MetricSettings `mapstructure:"databricks.spark.databricks.directory_commit.vacuum.errors"`
	DatabricksSparkDatabricksPreemptionChecksCount                                                                     MetricSettings `mapstructure:"databricks.spark.databricks.preemption.checks.count"`
	DatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount                                                           MetricSettings `mapstructure:"databricks.spark.databricks.preemption.pools_autoexpired.count"`
	DatabricksSparkDatabricksPreemptionPoolstarvationTime                                                              MetricSettings `mapstructure:"databricks.spark.databricks.preemption.poolstarvation.time"`
	DatabricksSparkDatabricksPreemptionSchedulerOverheadTime                                                           MetricSettings `mapstructure:"databricks.spark.databricks.preemption.scheduler_overhead.time"`
	DatabricksSparkDatabricksPreemptionTaskWastedTime                                                                  MetricSettings `mapstructure:"databricks.spark.databricks.preemption.task_wasted.time"`
	DatabricksSparkDatabricksPreemptionTasksPreemptedCount                                                             MetricSettings `mapstructure:"databricks.spark.databricks.preemption.tasks_preempted.count"`
	DatabricksSparkDatabricksTaskSchedulingLanesActivePools                                                            MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.active_pools"`
	DatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools                                                  MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.bypass_lane_active_pools"`
	DatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools                                                    MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.fast_lane_active_pools"`
	DatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime                                           MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.finished_queries_total_task.time"`
	DatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools                                                 MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.marked_pools"`
	DatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned                                        MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.two_phase_pools_cleaned"`
	DatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned                                          MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.zombie_pools_cleaned"`
	DatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount              MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_successful_preemption_iterations.count"`
	DatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount                              MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_tasks_preempted.count"`
	DatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime                                   MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_wasted_task.time"`
	DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount                                    MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.slot_reservation.gradual_decrease.count"`
	DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount                                          MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_drop.count"`
	DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount                                          MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_jump.count"`
	DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved                                           MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.slot_reservation.slots_reserved"`
	DatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools                                                    MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.slow_lane_active_pools"`
	DatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished                                               MetricSettings `mapstructure:"databricks.spark.databricks.task_scheduling_lanes.totalquerygroupsfinished"`
	DatabricksSparkExecutorDiskUsed                                                                                    MetricSettings `mapstructure:"databricks.spark.executor.disk_used"`
	DatabricksSparkExecutorMaxMemory                                                                                   MetricSettings `mapstructure:"databricks.spark.executor.max_memory"`
	DatabricksSparkExecutorMemoryUsed                                                                                  MetricSettings `mapstructure:"databricks.spark.executor.memory_used"`
	DatabricksSparkExecutorTotalInputBytes                                                                             MetricSettings `mapstructure:"databricks.spark.executor.total_input_bytes"`
	DatabricksSparkExecutorTotalShuffleRead                                                                            MetricSettings `mapstructure:"databricks.spark.executor.total_shuffle_read"`
	DatabricksSparkExecutorTotalShuffleWrite                                                                           MetricSettings `mapstructure:"databricks.spark.executor.total_shuffle_write"`
	DatabricksSparkExecutorMetricsDirectPoolMemory                                                                     MetricSettings `mapstructure:"databricks.spark.executor_metrics.direct_pool.memory"`
	DatabricksSparkExecutorMetricsJvmHeapMemory                                                                        MetricSettings `mapstructure:"databricks.spark.executor_metrics.jvm.heap.memory"`
	DatabricksSparkExecutorMetricsJvmOffHeapMemory                                                                     MetricSettings `mapstructure:"databricks.spark.executor_metrics.jvm.off_heap.memory"`
	DatabricksSparkExecutorMetricsMajorGcCount                                                                         MetricSettings `mapstructure:"databricks.spark.executor_metrics.major_gc.count"`
	DatabricksSparkExecutorMetricsMajorGcTime                                                                          MetricSettings `mapstructure:"databricks.spark.executor_metrics.major_gc.time"`
	DatabricksSparkExecutorMetricsMappedPoolMemory                                                                     MetricSettings `mapstructure:"databricks.spark.executor_metrics.mapped_pool.memory"`
	DatabricksSparkExecutorMetricsMinorGcCount                                                                         MetricSettings `mapstructure:"databricks.spark.executor_metrics.minor_gc.count"`
	DatabricksSparkExecutorMetricsMinorGcTime                                                                          MetricSettings `mapstructure:"databricks.spark.executor_metrics.minor_gc.time"`
	DatabricksSparkExecutorMetricsOffHeapExecutionMemory                                                               MetricSettings `mapstructure:"databricks.spark.executor_metrics.off_heap.execution.memory"`
	DatabricksSparkExecutorMetricsOffHeapStorageMemory                                                                 MetricSettings `mapstructure:"databricks.spark.executor_metrics.off_heap.storage.memory"`
	DatabricksSparkExecutorMetricsOffHeapUnifiedMemory                                                                 MetricSettings `mapstructure:"databricks.spark.executor_metrics.off_heap.unified.memory"`
	DatabricksSparkExecutorMetricsOnHeapExecutionMemory                                                                MetricSettings `mapstructure:"databricks.spark.executor_metrics.on_heap.execution.memory"`
	DatabricksSparkExecutorMetricsOnHeapStorageMemory                                                                  MetricSettings `mapstructure:"databricks.spark.executor_metrics.on_heap.storage.memory"`
	DatabricksSparkExecutorMetricsOnHeapUnifiedMemory                                                                  MetricSettings `mapstructure:"databricks.spark.executor_metrics.on_heap.unified.memory"`
	DatabricksSparkExecutorMetricsProcessTreeJvmRssMemory                                                              MetricSettings `mapstructure:"databricks.spark.executor_metrics.process_tree.jvm_rss.memory"`
	DatabricksSparkExecutorMetricsProcessTreeJvmVMemory                                                                MetricSettings `mapstructure:"databricks.spark.executor_metrics.process_tree.jvm_v.memory"`
	DatabricksSparkExecutorMetricsProcessTreeOtherRssMemory                                                            MetricSettings `mapstructure:"databricks.spark.executor_metrics.process_tree.other_rss.memory"`
	DatabricksSparkExecutorMetricsProcessTreeOtherVMemory                                                              MetricSettings `mapstructure:"databricks.spark.executor_metrics.process_tree.other_v.memory"`
	DatabricksSparkExecutorMetricsProcessTreePythonRssMemory                                                           MetricSettings `mapstructure:"databricks.spark.executor_metrics.process_tree.python_rss.memory"`
	DatabricksSparkExecutorMetricsProcessTreePythonVMemory                                                             MetricSettings `mapstructure:"databricks.spark.executor_metrics.process_tree.python_v.memory"`
	DatabricksSparkHiveExternalCatalogFileCacheHits                                                                    MetricSettings `mapstructure:"databricks.spark.hive_external_catalog.file_cache.hits"`
	DatabricksSparkHiveExternalCatalogFilesDiscovered                                                                  MetricSettings `mapstructure:"databricks.spark.hive_external_catalog.files_discovered"`
	DatabricksSparkHiveExternalCatalogHiveClientCalls                                                                  MetricSettings `mapstructure:"databricks.spark.hive_external_catalog.hive_client_calls"`
	DatabricksSparkHiveExternalCatalogParallelListingJobsCount                                                         MetricSettings `mapstructure:"databricks.spark.hive_external_catalog.parallel_listing_jobs.count"`
	DatabricksSparkHiveExternalCatalogPartitionsFetched                                                                MetricSettings `mapstructure:"databricks.spark.hive_external_catalog.partitions_fetched"`
	DatabricksSparkJobNumActiveStages                                                                                  MetricSettings `mapstructure:"databricks.spark.job.num_active_stages"`
	DatabricksSparkJobNumActiveTasks                                                                                   MetricSettings `mapstructure:"databricks.spark.job.num_active_tasks"`
	DatabricksSparkJobNumCompletedStages                                                                               MetricSettings `mapstructure:"databricks.spark.job.num_completed_stages"`
	DatabricksSparkJobNumCompletedTasks                                                                                MetricSettings `mapstructure:"databricks.spark.job.num_completed_tasks"`
	DatabricksSparkJobNumFailedStages                                                                                  MetricSettings `mapstructure:"databricks.spark.job.num_failed_stages"`
	DatabricksSparkJobNumFailedTasks                                                                                   MetricSettings `mapstructure:"databricks.spark.job.num_failed_tasks"`
	DatabricksSparkJobNumSkippedStages                                                                                 MetricSettings `mapstructure:"databricks.spark.job.num_skipped_stages"`
	DatabricksSparkJobNumSkippedTasks                                                                                  MetricSettings `mapstructure:"databricks.spark.job.num_skipped_tasks"`
	DatabricksSparkJobNumTasks                                                                                         MetricSettings `mapstructure:"databricks.spark.job.num_tasks"`
	DatabricksSparkJvmCPUTime                                                                                          MetricSettings `mapstructure:"databricks.spark.jvm.cpu.time"`
	DatabricksSparkLiveListenerBusEventsPostedCount                                                                    MetricSettings `mapstructure:"databricks.spark.live_listener_bus.events_posted.count"`
	DatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount                                                     MetricSettings `mapstructure:"databricks.spark.live_listener_bus.queue.app_status.dropped_events.count"`
	DatabricksSparkLiveListenerBusQueueAppstatusSize                                                                   MetricSettings `mapstructure:"databricks.spark.live_listener_bus.queue.appstatus.size"`
	DatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount                                            MetricSettings `mapstructure:"databricks.spark.live_listener_bus.queue.executor_management.dropped_events.count"`
	DatabricksSparkLiveListenerBusQueueExecutormanagementSize                                                          MetricSettings `mapstructure:"databricks.spark.live_listener_bus.queue.executormanagement.size"`
	DatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount                                                        MetricSettings `mapstructure:"databricks.spark.live_listener_bus.queue.shared.dropped_events.count"`
	DatabricksSparkLiveListenerBusQueueSharedSize                                                                      MetricSettings `mapstructure:"databricks.spark.live_listener_bus.queue.shared.size"`
	DatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount                                                       MetricSettings `mapstructure:"databricks.spark.live_listener_bus.queue.streams.dropped_events.count"`
	DatabricksSparkLiveListenerBusQueueStreamsSize                                                                     MetricSettings `mapstructure:"databricks.spark.live_listener_bus.queue.streams.size"`
	DatabricksSparkSparkSQLOperationManagerHiveOperationsCount                                                         MetricSettings `mapstructure:"databricks.spark.spark_sql_operation_manager.hive_operations.count"`
	DatabricksSparkStageDiskBytesSpilled                                                                               MetricSettings `mapstructure:"databricks.spark.stage.disk_bytes_spilled"`
	DatabricksSparkStageExecutorRunTime                                                                                MetricSettings `mapstructure:"databricks.spark.stage.executor_run_time"`
	DatabricksSparkStageInputBytes                                                                                     MetricSettings `mapstructure:"databricks.spark.stage.input_bytes"`
	DatabricksSparkStageInputRecords                                                                                   MetricSettings `mapstructure:"databricks.spark.stage.input_records"`
	DatabricksSparkStageMemoryBytesSpilled                                                                             MetricSettings `mapstructure:"databricks.spark.stage.memory_bytes_spilled"`
	DatabricksSparkStageOutputBytes                                                                                    MetricSettings `mapstructure:"databricks.spark.stage.output_bytes"`
	DatabricksSparkStageOutputRecords                                                                                  MetricSettings `mapstructure:"databricks.spark.stage.output_records"`
	DatabricksSparkTimerDagSchedulerMessageProcessingTime                                                              MetricSettings `mapstructure:"databricks.spark.timer.dag_scheduler.message_processing.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime          MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.streaming.query_listener_bus.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime                                   MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime             MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.ui.sql_app_status_listener.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.hive.thriftserver.ui.hive_thrift_server2listener.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime                                MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.spark_session.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime                    MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.util.execution_listener_bus.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime                        MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.status.app_status_listener.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime                                MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.util.profiler_env.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime       MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.data_plane_event_listener.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime      MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.dbc_event_logging_listener.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime                     MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.databricks.photon.photon_cleanup_listener.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime            MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.executor_time_logging_listener.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime                   MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.usage_logging_listener.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime                        MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.advice.advisor_listener.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime                MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.debugger.query_watchdog_listener.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime                   MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.execution.ui.io_cache_listener.time"`
	DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime              MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.io.caching.repeated_reads_estimator.time"`
	DatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime                                            MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.queue.app_status.listener_processing.time"`
	DatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime                                   MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.queue.executor_management.listener_processing.time"`
	DatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime                                               MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.queue.shared.listener_processing.time"`
	DatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime                                              MetricSettings `mapstructure:"databricks.spark.timer.live_listener_bus.queue.streams.listener_processing.time"`
	DatabricksTasksRunDuration                                                                                         MetricSettings `mapstructure:"databricks.tasks.run.duration"`
	DatabricksTasksScheduleStatus                                                                                      MetricSettings `mapstructure:"databricks.tasks.schedule.status"`
}

func DefaultMetricsSettings() MetricsSettings {
	return MetricsSettings{
		DatabricksJobsActiveTotal: MetricSettings{
			Enabled: true,
		},
		DatabricksJobsRunDuration: MetricSettings{
			Enabled: true,
		},
		DatabricksJobsScheduleStatus: MetricSettings{
			Enabled: true,
		},
		DatabricksJobsTotal: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkBlockManagerMemoryDiskSpaceUsed: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkBlockManagerMemoryMax: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkBlockManagerMemoryOffHeapMax: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkBlockManagerMemoryOffHeapUsed: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkBlockManagerMemoryOnHeapMax: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkBlockManagerMemoryOnHeapUsed: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkBlockManagerMemoryRemaining: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkBlockManagerMemoryRemainingOffHeap: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkBlockManagerMemoryRemainingOnHeap: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkBlockManagerMemoryUsed: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkCodeGeneratorCompilationTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkCodeGeneratorGeneratedClassSize: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkCodeGeneratorGeneratedMethodSize: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkCodeGeneratorSourcecodeSize: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDagSchedulerJobsActive: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDagSchedulerJobsAll: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDagSchedulerStagesFailed: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDagSchedulerStagesRunning: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDagSchedulerStagesWaiting: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitAutoVacuumCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitFilterListingCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitJobCommitCompleted: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitMarkerReadErrors: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitMarkersRead: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitRepeatedListCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitVacuumCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksDirectoryCommitVacuumErrors: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksPreemptionChecksCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksPreemptionPoolstarvationTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksPreemptionSchedulerOverheadTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksPreemptionTaskWastedTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksPreemptionTasksPreemptedCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesActivePools: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorDiskUsed: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMaxMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMemoryUsed: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorTotalInputBytes: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorTotalShuffleRead: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorTotalShuffleWrite: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsDirectPoolMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsJvmHeapMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsJvmOffHeapMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsMajorGcCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsMajorGcTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsMappedPoolMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsMinorGcCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsMinorGcTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsOffHeapExecutionMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsOffHeapStorageMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsOffHeapUnifiedMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsOnHeapExecutionMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsOnHeapStorageMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsOnHeapUnifiedMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsProcessTreeJvmRssMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsProcessTreeJvmVMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsProcessTreeOtherRssMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsProcessTreeOtherVMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsProcessTreePythonRssMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkExecutorMetricsProcessTreePythonVMemory: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkHiveExternalCatalogFileCacheHits: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkHiveExternalCatalogFilesDiscovered: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkHiveExternalCatalogHiveClientCalls: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkHiveExternalCatalogParallelListingJobsCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkHiveExternalCatalogPartitionsFetched: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkJobNumActiveStages: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkJobNumActiveTasks: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkJobNumCompletedStages: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkJobNumCompletedTasks: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkJobNumFailedStages: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkJobNumFailedTasks: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkJobNumSkippedStages: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkJobNumSkippedTasks: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkJobNumTasks: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkJvmCPUTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkLiveListenerBusEventsPostedCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkLiveListenerBusQueueAppstatusSize: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkLiveListenerBusQueueExecutormanagementSize: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkLiveListenerBusQueueSharedSize: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkLiveListenerBusQueueStreamsSize: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkSparkSQLOperationManagerHiveOperationsCount: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkStageDiskBytesSpilled: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkStageExecutorRunTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkStageInputBytes: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkStageInputRecords: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkStageMemoryBytesSpilled: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkStageOutputBytes: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkStageOutputRecords: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerDagSchedulerMessageProcessingTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime: MetricSettings{
			Enabled: true,
		},
		DatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime: MetricSettings{
			Enabled: true,
		},
		DatabricksTasksRunDuration: MetricSettings{
			Enabled: true,
		},
		DatabricksTasksScheduleStatus: MetricSettings{
			Enabled: true,
		},
	}
}

// AttributeTaskType specifies the a value task.type attribute.
type AttributeTaskType int

const (
	_ AttributeTaskType = iota
	AttributeTaskTypeNotebookTask
	AttributeTaskTypeSparkJarTask
	AttributeTaskTypeSparkPythonTask
	AttributeTaskTypePipelineTask
	AttributeTaskTypePythonWheelTask
	AttributeTaskTypeSparkSubmitTask
)

// String returns the string representation of the AttributeTaskType.
func (av AttributeTaskType) String() string {
	switch av {
	case AttributeTaskTypeNotebookTask:
		return "NotebookTask"
	case AttributeTaskTypeSparkJarTask:
		return "SparkJarTask"
	case AttributeTaskTypeSparkPythonTask:
		return "SparkPythonTask"
	case AttributeTaskTypePipelineTask:
		return "PipelineTask"
	case AttributeTaskTypePythonWheelTask:
		return "PythonWheelTask"
	case AttributeTaskTypeSparkSubmitTask:
		return "SparkSubmitTask"
	}
	return ""
}

// MapAttributeTaskType is a helper map of string to AttributeTaskType attribute value.
var MapAttributeTaskType = map[string]AttributeTaskType{
	"NotebookTask":    AttributeTaskTypeNotebookTask,
	"SparkJarTask":    AttributeTaskTypeSparkJarTask,
	"SparkPythonTask": AttributeTaskTypeSparkPythonTask,
	"PipelineTask":    AttributeTaskTypePipelineTask,
	"PythonWheelTask": AttributeTaskTypePythonWheelTask,
	"SparkSubmitTask": AttributeTaskTypeSparkSubmitTask,
}

type metricDatabricksJobsActiveTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.jobs.active.total metric with initial data.
func (m *metricDatabricksJobsActiveTotal) init() {
	m.data.SetName("databricks.jobs.active.total")
	m.data.SetDescription("A snapshot of the number of active jobs taken at each scrape")
	m.data.SetUnit("{jobs}")
	m.data.SetEmptyGauge()
}

func (m *metricDatabricksJobsActiveTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksJobsActiveTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksJobsActiveTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksJobsActiveTotal(settings MetricSettings) metricDatabricksJobsActiveTotal {
	m := metricDatabricksJobsActiveTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksJobsRunDuration struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.jobs.run.duration metric with initial data.
func (m *metricDatabricksJobsRunDuration) init() {
	m.data.SetName("databricks.jobs.run.duration")
	m.data.SetDescription("The execution duration in milliseconds per completed job")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksJobsRunDuration) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutInt("job.id", jobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksJobsRunDuration) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksJobsRunDuration) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksJobsRunDuration(settings MetricSettings) metricDatabricksJobsRunDuration {
	m := metricDatabricksJobsRunDuration{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksJobsScheduleStatus struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.jobs.schedule.status metric with initial data.
func (m *metricDatabricksJobsScheduleStatus) init() {
	m.data.SetName("databricks.jobs.schedule.status")
	m.data.SetDescription("A snapshot of the pause/run status per job taken at each scrape")
	m.data.SetUnit("{status}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksJobsScheduleStatus) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutInt("job.id", jobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksJobsScheduleStatus) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksJobsScheduleStatus) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksJobsScheduleStatus(settings MetricSettings) metricDatabricksJobsScheduleStatus {
	m := metricDatabricksJobsScheduleStatus{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksJobsTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.jobs.total metric with initial data.
func (m *metricDatabricksJobsTotal) init() {
	m.data.SetName("databricks.jobs.total")
	m.data.SetDescription("A snapshot of the total number of jobs registered in the Databricks instance taken at each scrape")
	m.data.SetUnit("{jobs}")
	m.data.SetEmptyGauge()
}

func (m *metricDatabricksJobsTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksJobsTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksJobsTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksJobsTotal(settings MetricSettings) metricDatabricksJobsTotal {
	m := metricDatabricksJobsTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.block_manager.memory.disk_space.used metric with initial data.
func (m *metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed) init() {
	m.data.SetName("databricks.spark.block_manager.memory.disk_space.used")
	m.data.SetDescription("n/a")
	m.data.SetUnit("mb")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkBlockManagerMemoryDiskSpaceUsed(settings MetricSettings) metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed {
	m := metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkBlockManagerMemoryMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.block_manager.memory.max metric with initial data.
func (m *metricDatabricksSparkBlockManagerMemoryMax) init() {
	m.data.SetName("databricks.spark.block_manager.memory.max")
	m.data.SetDescription("n/a")
	m.data.SetUnit("mb")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkBlockManagerMemoryMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkBlockManagerMemoryMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkBlockManagerMemoryMax) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkBlockManagerMemoryMax(settings MetricSettings) metricDatabricksSparkBlockManagerMemoryMax {
	m := metricDatabricksSparkBlockManagerMemoryMax{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkBlockManagerMemoryOffHeapMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.block_manager.memory.off_heap.max metric with initial data.
func (m *metricDatabricksSparkBlockManagerMemoryOffHeapMax) init() {
	m.data.SetName("databricks.spark.block_manager.memory.off_heap.max")
	m.data.SetDescription("n/a")
	m.data.SetUnit("mb")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkBlockManagerMemoryOffHeapMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkBlockManagerMemoryOffHeapMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkBlockManagerMemoryOffHeapMax) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkBlockManagerMemoryOffHeapMax(settings MetricSettings) metricDatabricksSparkBlockManagerMemoryOffHeapMax {
	m := metricDatabricksSparkBlockManagerMemoryOffHeapMax{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkBlockManagerMemoryOffHeapUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.block_manager.memory.off_heap.used metric with initial data.
func (m *metricDatabricksSparkBlockManagerMemoryOffHeapUsed) init() {
	m.data.SetName("databricks.spark.block_manager.memory.off_heap.used")
	m.data.SetDescription("n/a")
	m.data.SetUnit("mb")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkBlockManagerMemoryOffHeapUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkBlockManagerMemoryOffHeapUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkBlockManagerMemoryOffHeapUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkBlockManagerMemoryOffHeapUsed(settings MetricSettings) metricDatabricksSparkBlockManagerMemoryOffHeapUsed {
	m := metricDatabricksSparkBlockManagerMemoryOffHeapUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkBlockManagerMemoryOnHeapMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.block_manager.memory.on_heap.max metric with initial data.
func (m *metricDatabricksSparkBlockManagerMemoryOnHeapMax) init() {
	m.data.SetName("databricks.spark.block_manager.memory.on_heap.max")
	m.data.SetDescription("n/a")
	m.data.SetUnit("mb")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkBlockManagerMemoryOnHeapMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkBlockManagerMemoryOnHeapMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkBlockManagerMemoryOnHeapMax) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkBlockManagerMemoryOnHeapMax(settings MetricSettings) metricDatabricksSparkBlockManagerMemoryOnHeapMax {
	m := metricDatabricksSparkBlockManagerMemoryOnHeapMax{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkBlockManagerMemoryOnHeapUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.block_manager.memory.on_heap.used metric with initial data.
func (m *metricDatabricksSparkBlockManagerMemoryOnHeapUsed) init() {
	m.data.SetName("databricks.spark.block_manager.memory.on_heap.used")
	m.data.SetDescription("n/a")
	m.data.SetUnit("mb")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkBlockManagerMemoryOnHeapUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkBlockManagerMemoryOnHeapUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkBlockManagerMemoryOnHeapUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkBlockManagerMemoryOnHeapUsed(settings MetricSettings) metricDatabricksSparkBlockManagerMemoryOnHeapUsed {
	m := metricDatabricksSparkBlockManagerMemoryOnHeapUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkBlockManagerMemoryRemaining struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.block_manager.memory.remaining metric with initial data.
func (m *metricDatabricksSparkBlockManagerMemoryRemaining) init() {
	m.data.SetName("databricks.spark.block_manager.memory.remaining")
	m.data.SetDescription("n/a")
	m.data.SetUnit("mb")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkBlockManagerMemoryRemaining) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkBlockManagerMemoryRemaining) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkBlockManagerMemoryRemaining) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkBlockManagerMemoryRemaining(settings MetricSettings) metricDatabricksSparkBlockManagerMemoryRemaining {
	m := metricDatabricksSparkBlockManagerMemoryRemaining{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkBlockManagerMemoryRemainingOffHeap struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.block_manager.memory.remaining.off_heap metric with initial data.
func (m *metricDatabricksSparkBlockManagerMemoryRemainingOffHeap) init() {
	m.data.SetName("databricks.spark.block_manager.memory.remaining.off_heap")
	m.data.SetDescription("n/a")
	m.data.SetUnit("mb")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkBlockManagerMemoryRemainingOffHeap) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkBlockManagerMemoryRemainingOffHeap) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkBlockManagerMemoryRemainingOffHeap) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkBlockManagerMemoryRemainingOffHeap(settings MetricSettings) metricDatabricksSparkBlockManagerMemoryRemainingOffHeap {
	m := metricDatabricksSparkBlockManagerMemoryRemainingOffHeap{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkBlockManagerMemoryRemainingOnHeap struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.block_manager.memory.remaining.on_heap metric with initial data.
func (m *metricDatabricksSparkBlockManagerMemoryRemainingOnHeap) init() {
	m.data.SetName("databricks.spark.block_manager.memory.remaining.on_heap")
	m.data.SetDescription("n/a")
	m.data.SetUnit("mb")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkBlockManagerMemoryRemainingOnHeap) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkBlockManagerMemoryRemainingOnHeap) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkBlockManagerMemoryRemainingOnHeap) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkBlockManagerMemoryRemainingOnHeap(settings MetricSettings) metricDatabricksSparkBlockManagerMemoryRemainingOnHeap {
	m := metricDatabricksSparkBlockManagerMemoryRemainingOnHeap{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkBlockManagerMemoryUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.block_manager.memory.used metric with initial data.
func (m *metricDatabricksSparkBlockManagerMemoryUsed) init() {
	m.data.SetName("databricks.spark.block_manager.memory.used")
	m.data.SetDescription("n/a")
	m.data.SetUnit("mb")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkBlockManagerMemoryUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkBlockManagerMemoryUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkBlockManagerMemoryUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkBlockManagerMemoryUsed(settings MetricSettings) metricDatabricksSparkBlockManagerMemoryUsed {
	m := metricDatabricksSparkBlockManagerMemoryUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkCodeGeneratorCompilationTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.code_generator.compilation.time metric with initial data.
func (m *metricDatabricksSparkCodeGeneratorCompilationTime) init() {
	m.data.SetName("databricks.spark.code_generator.compilation.time")
	m.data.SetDescription("This value comes from the 'mean' field in a histogram returned by the /metrics/json/ endpoint.")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkCodeGeneratorCompilationTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkCodeGeneratorCompilationTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkCodeGeneratorCompilationTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkCodeGeneratorCompilationTime(settings MetricSettings) metricDatabricksSparkCodeGeneratorCompilationTime {
	m := metricDatabricksSparkCodeGeneratorCompilationTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkCodeGeneratorGeneratedClassSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.code_generator.generated_class_size metric with initial data.
func (m *metricDatabricksSparkCodeGeneratorGeneratedClassSize) init() {
	m.data.SetName("databricks.spark.code_generator.generated_class_size")
	m.data.SetDescription("This value comes from the 'mean' field in a histogram returned by the /metrics/json/ endpoint.")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkCodeGeneratorGeneratedClassSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkCodeGeneratorGeneratedClassSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkCodeGeneratorGeneratedClassSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkCodeGeneratorGeneratedClassSize(settings MetricSettings) metricDatabricksSparkCodeGeneratorGeneratedClassSize {
	m := metricDatabricksSparkCodeGeneratorGeneratedClassSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkCodeGeneratorGeneratedMethodSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.code_generator.generated_method_size metric with initial data.
func (m *metricDatabricksSparkCodeGeneratorGeneratedMethodSize) init() {
	m.data.SetName("databricks.spark.code_generator.generated_method_size")
	m.data.SetDescription("This value comes from the 'mean' field in a histogram returned by the /metrics/json/ endpoint.")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkCodeGeneratorGeneratedMethodSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkCodeGeneratorGeneratedMethodSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkCodeGeneratorGeneratedMethodSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkCodeGeneratorGeneratedMethodSize(settings MetricSettings) metricDatabricksSparkCodeGeneratorGeneratedMethodSize {
	m := metricDatabricksSparkCodeGeneratorGeneratedMethodSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkCodeGeneratorSourcecodeSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.code_generator.sourcecode_size metric with initial data.
func (m *metricDatabricksSparkCodeGeneratorSourcecodeSize) init() {
	m.data.SetName("databricks.spark.code_generator.sourcecode_size")
	m.data.SetDescription("This value comes from the 'mean' field in a histogram returned by the /metrics/json/ endpoint.")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkCodeGeneratorSourcecodeSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkCodeGeneratorSourcecodeSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkCodeGeneratorSourcecodeSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkCodeGeneratorSourcecodeSize(settings MetricSettings) metricDatabricksSparkCodeGeneratorSourcecodeSize {
	m := metricDatabricksSparkCodeGeneratorSourcecodeSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDagSchedulerJobsActive struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.dag_scheduler.jobs.active metric with initial data.
func (m *metricDatabricksSparkDagSchedulerJobsActive) init() {
	m.data.SetName("databricks.spark.dag_scheduler.jobs.active")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDagSchedulerJobsActive) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDagSchedulerJobsActive) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDagSchedulerJobsActive) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDagSchedulerJobsActive(settings MetricSettings) metricDatabricksSparkDagSchedulerJobsActive {
	m := metricDatabricksSparkDagSchedulerJobsActive{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDagSchedulerJobsAll struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.dag_scheduler.jobs.all metric with initial data.
func (m *metricDatabricksSparkDagSchedulerJobsAll) init() {
	m.data.SetName("databricks.spark.dag_scheduler.jobs.all")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDagSchedulerJobsAll) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDagSchedulerJobsAll) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDagSchedulerJobsAll) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDagSchedulerJobsAll(settings MetricSettings) metricDatabricksSparkDagSchedulerJobsAll {
	m := metricDatabricksSparkDagSchedulerJobsAll{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDagSchedulerStagesFailed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.dag_scheduler.stages.failed metric with initial data.
func (m *metricDatabricksSparkDagSchedulerStagesFailed) init() {
	m.data.SetName("databricks.spark.dag_scheduler.stages.failed")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDagSchedulerStagesFailed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDagSchedulerStagesFailed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDagSchedulerStagesFailed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDagSchedulerStagesFailed(settings MetricSettings) metricDatabricksSparkDagSchedulerStagesFailed {
	m := metricDatabricksSparkDagSchedulerStagesFailed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDagSchedulerStagesRunning struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.dag_scheduler.stages.running metric with initial data.
func (m *metricDatabricksSparkDagSchedulerStagesRunning) init() {
	m.data.SetName("databricks.spark.dag_scheduler.stages.running")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDagSchedulerStagesRunning) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDagSchedulerStagesRunning) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDagSchedulerStagesRunning) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDagSchedulerStagesRunning(settings MetricSettings) metricDatabricksSparkDagSchedulerStagesRunning {
	m := metricDatabricksSparkDagSchedulerStagesRunning{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDagSchedulerStagesWaiting struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.dag_scheduler.stages.waiting metric with initial data.
func (m *metricDatabricksSparkDagSchedulerStagesWaiting) init() {
	m.data.SetName("databricks.spark.dag_scheduler.stages.waiting")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDagSchedulerStagesWaiting) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDagSchedulerStagesWaiting) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDagSchedulerStagesWaiting) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDagSchedulerStagesWaiting(settings MetricSettings) metricDatabricksSparkDagSchedulerStagesWaiting {
	m := metricDatabricksSparkDagSchedulerStagesWaiting{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.auto_vacuum.count metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.auto_vacuum.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount {
	m := metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.deleted_files_filtered metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.deleted_files_filtered")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered {
	m := metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.filter_listing.count metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.filter_listing.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitFilterListingCount(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount {
	m := metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.job_commit_completed metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.job_commit_completed")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted {
	m := metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.marker_read.errors metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.marker_read.errors")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors {
	m := metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.marker_refresh.count metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.marker_refresh.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount {
	m := metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.marker_refresh.errors metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.marker_refresh.errors")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors {
	m := metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitMarkersRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.markers.read metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkersRead) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.markers.read")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkersRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkersRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitMarkersRead) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitMarkersRead(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitMarkersRead {
	m := metricDatabricksSparkDatabricksDirectoryCommitMarkersRead{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.repeated_list.count metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.repeated_list.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount {
	m := metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.uncommitted_files.filtered metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.uncommitted_files.filtered")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered {
	m := metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.untracked_files.found metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.untracked_files.found")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound {
	m := metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitVacuumCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.vacuum.count metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitVacuumCount) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.vacuum.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitVacuumCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitVacuumCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitVacuumCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitVacuumCount(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitVacuumCount {
	m := metricDatabricksSparkDatabricksDirectoryCommitVacuumCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.directory_commit.vacuum.errors metric with initial data.
func (m *metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors) init() {
	m.data.SetName("databricks.spark.databricks.directory_commit.vacuum.errors")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksDirectoryCommitVacuumErrors(settings MetricSettings) metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors {
	m := metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksPreemptionChecksCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.preemption.checks.count metric with initial data.
func (m *metricDatabricksSparkDatabricksPreemptionChecksCount) init() {
	m.data.SetName("databricks.spark.databricks.preemption.checks.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksPreemptionChecksCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksPreemptionChecksCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksPreemptionChecksCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksPreemptionChecksCount(settings MetricSettings) metricDatabricksSparkDatabricksPreemptionChecksCount {
	m := metricDatabricksSparkDatabricksPreemptionChecksCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.preemption.pools_autoexpired.count metric with initial data.
func (m *metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount) init() {
	m.data.SetName("databricks.spark.databricks.preemption.pools_autoexpired.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount(settings MetricSettings) metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount {
	m := metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksPreemptionPoolstarvationTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.preemption.poolstarvation.time metric with initial data.
func (m *metricDatabricksSparkDatabricksPreemptionPoolstarvationTime) init() {
	m.data.SetName("databricks.spark.databricks.preemption.poolstarvation.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksPreemptionPoolstarvationTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksPreemptionPoolstarvationTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksPreemptionPoolstarvationTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksPreemptionPoolstarvationTime(settings MetricSettings) metricDatabricksSparkDatabricksPreemptionPoolstarvationTime {
	m := metricDatabricksSparkDatabricksPreemptionPoolstarvationTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.preemption.scheduler_overhead.time metric with initial data.
func (m *metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime) init() {
	m.data.SetName("databricks.spark.databricks.preemption.scheduler_overhead.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ns")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime(settings MetricSettings) metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime {
	m := metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksPreemptionTaskWastedTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.preemption.task_wasted.time metric with initial data.
func (m *metricDatabricksSparkDatabricksPreemptionTaskWastedTime) init() {
	m.data.SetName("databricks.spark.databricks.preemption.task_wasted.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksPreemptionTaskWastedTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksPreemptionTaskWastedTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksPreemptionTaskWastedTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksPreemptionTaskWastedTime(settings MetricSettings) metricDatabricksSparkDatabricksPreemptionTaskWastedTime {
	m := metricDatabricksSparkDatabricksPreemptionTaskWastedTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.preemption.tasks_preempted.count metric with initial data.
func (m *metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount) init() {
	m.data.SetName("databricks.spark.databricks.preemption.tasks_preempted.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksPreemptionTasksPreemptedCount(settings MetricSettings) metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount {
	m := metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.active_pools metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.active_pools")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesActivePools(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.bypass_lane_active_pools metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.bypass_lane_active_pools")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.fast_lane_active_pools metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.fast_lane_active_pools")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.finished_queries_total_task.time metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.finished_queries_total_task.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ns")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.marked_pools metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.marked_pools")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.two_phase_pools_cleaned metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.two_phase_pools_cleaned")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.zombie_pools_cleaned metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.zombie_pools_cleaned")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_successful_preemption_iterations.count metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_successful_preemption_iterations.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_tasks_preempted.count metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_tasks_preempted.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_wasted_task.time metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_wasted_task.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ns")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.slot_reservation.gradual_decrease.count metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.slot_reservation.gradual_decrease.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_drop.count metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_drop.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_jump.count metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_jump.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.slot_reservation.slots_reserved metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.slot_reservation.slots_reserved")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.slow_lane_active_pools metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.slow_lane_active_pools")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.databricks.task_scheduling_lanes.totalquerygroupsfinished metric with initial data.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished) init() {
	m.data.SetName("databricks.spark.databricks.task_scheduling_lanes.totalquerygroupsfinished")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished(settings MetricSettings) metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished {
	m := metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorDiskUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor.disk_used metric with initial data.
func (m *metricDatabricksSparkExecutorDiskUsed) init() {
	m.data.SetName("databricks.spark.executor.disk_used")
	m.data.SetDescription("n/a")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorDiskUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("spark.executor.id", sparkExecutorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorDiskUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorDiskUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorDiskUsed(settings MetricSettings) metricDatabricksSparkExecutorDiskUsed {
	m := metricDatabricksSparkExecutorDiskUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMaxMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor.max_memory metric with initial data.
func (m *metricDatabricksSparkExecutorMaxMemory) init() {
	m.data.SetName("databricks.spark.executor.max_memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMaxMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("spark.executor.id", sparkExecutorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMaxMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMaxMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMaxMemory(settings MetricSettings) metricDatabricksSparkExecutorMaxMemory {
	m := metricDatabricksSparkExecutorMaxMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMemoryUsed struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor.memory_used metric with initial data.
func (m *metricDatabricksSparkExecutorMemoryUsed) init() {
	m.data.SetName("databricks.spark.executor.memory_used")
	m.data.SetDescription("n/a")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMemoryUsed) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("spark.executor.id", sparkExecutorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMemoryUsed) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMemoryUsed) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMemoryUsed(settings MetricSettings) metricDatabricksSparkExecutorMemoryUsed {
	m := metricDatabricksSparkExecutorMemoryUsed{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorTotalInputBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor.total_input_bytes metric with initial data.
func (m *metricDatabricksSparkExecutorTotalInputBytes) init() {
	m.data.SetName("databricks.spark.executor.total_input_bytes")
	m.data.SetDescription("n/a")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorTotalInputBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("spark.executor.id", sparkExecutorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorTotalInputBytes) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorTotalInputBytes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorTotalInputBytes(settings MetricSettings) metricDatabricksSparkExecutorTotalInputBytes {
	m := metricDatabricksSparkExecutorTotalInputBytes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorTotalShuffleRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor.total_shuffle_read metric with initial data.
func (m *metricDatabricksSparkExecutorTotalShuffleRead) init() {
	m.data.SetName("databricks.spark.executor.total_shuffle_read")
	m.data.SetDescription("n/a")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorTotalShuffleRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("spark.executor.id", sparkExecutorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorTotalShuffleRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorTotalShuffleRead) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorTotalShuffleRead(settings MetricSettings) metricDatabricksSparkExecutorTotalShuffleRead {
	m := metricDatabricksSparkExecutorTotalShuffleRead{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorTotalShuffleWrite struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor.total_shuffle_write metric with initial data.
func (m *metricDatabricksSparkExecutorTotalShuffleWrite) init() {
	m.data.SetName("databricks.spark.executor.total_shuffle_write")
	m.data.SetDescription("n/a")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorTotalShuffleWrite) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("spark.executor.id", sparkExecutorIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorTotalShuffleWrite) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorTotalShuffleWrite) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorTotalShuffleWrite(settings MetricSettings) metricDatabricksSparkExecutorTotalShuffleWrite {
	m := metricDatabricksSparkExecutorTotalShuffleWrite{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsDirectPoolMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.direct_pool.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsDirectPoolMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.direct_pool.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsDirectPoolMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsDirectPoolMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsDirectPoolMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsDirectPoolMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsDirectPoolMemory {
	m := metricDatabricksSparkExecutorMetricsDirectPoolMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsJvmHeapMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.jvm.heap.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsJvmHeapMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.jvm.heap.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsJvmHeapMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsJvmHeapMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsJvmHeapMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsJvmHeapMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsJvmHeapMemory {
	m := metricDatabricksSparkExecutorMetricsJvmHeapMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsJvmOffHeapMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.jvm.off_heap.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsJvmOffHeapMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.jvm.off_heap.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsJvmOffHeapMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsJvmOffHeapMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsJvmOffHeapMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsJvmOffHeapMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsJvmOffHeapMemory {
	m := metricDatabricksSparkExecutorMetricsJvmOffHeapMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsMajorGcCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.major_gc.count metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsMajorGcCount) init() {
	m.data.SetName("databricks.spark.executor_metrics.major_gc.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsMajorGcCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsMajorGcCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsMajorGcCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsMajorGcCount(settings MetricSettings) metricDatabricksSparkExecutorMetricsMajorGcCount {
	m := metricDatabricksSparkExecutorMetricsMajorGcCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsMajorGcTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.major_gc.time metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsMajorGcTime) init() {
	m.data.SetName("databricks.spark.executor_metrics.major_gc.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsMajorGcTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsMajorGcTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsMajorGcTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsMajorGcTime(settings MetricSettings) metricDatabricksSparkExecutorMetricsMajorGcTime {
	m := metricDatabricksSparkExecutorMetricsMajorGcTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsMappedPoolMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.mapped_pool.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsMappedPoolMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.mapped_pool.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsMappedPoolMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsMappedPoolMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsMappedPoolMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsMappedPoolMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsMappedPoolMemory {
	m := metricDatabricksSparkExecutorMetricsMappedPoolMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsMinorGcCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.minor_gc.count metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsMinorGcCount) init() {
	m.data.SetName("databricks.spark.executor_metrics.minor_gc.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsMinorGcCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsMinorGcCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsMinorGcCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsMinorGcCount(settings MetricSettings) metricDatabricksSparkExecutorMetricsMinorGcCount {
	m := metricDatabricksSparkExecutorMetricsMinorGcCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsMinorGcTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.minor_gc.time metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsMinorGcTime) init() {
	m.data.SetName("databricks.spark.executor_metrics.minor_gc.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsMinorGcTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsMinorGcTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsMinorGcTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsMinorGcTime(settings MetricSettings) metricDatabricksSparkExecutorMetricsMinorGcTime {
	m := metricDatabricksSparkExecutorMetricsMinorGcTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.off_heap.execution.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.off_heap.execution.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsOffHeapExecutionMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory {
	m := metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsOffHeapStorageMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.off_heap.storage.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsOffHeapStorageMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.off_heap.storage.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsOffHeapStorageMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsOffHeapStorageMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsOffHeapStorageMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsOffHeapStorageMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsOffHeapStorageMemory {
	m := metricDatabricksSparkExecutorMetricsOffHeapStorageMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.off_heap.unified.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.off_heap.unified.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory {
	m := metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.on_heap.execution.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.on_heap.execution.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsOnHeapExecutionMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory {
	m := metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsOnHeapStorageMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.on_heap.storage.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsOnHeapStorageMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.on_heap.storage.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsOnHeapStorageMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsOnHeapStorageMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsOnHeapStorageMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsOnHeapStorageMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsOnHeapStorageMemory {
	m := metricDatabricksSparkExecutorMetricsOnHeapStorageMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.on_heap.unified.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.on_heap.unified.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory {
	m := metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.process_tree.jvm_rss.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.process_tree.jvm_rss.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory {
	m := metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.process_tree.jvm_v.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.process_tree.jvm_v.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory {
	m := metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.process_tree.other_rss.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.process_tree.other_rss.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory {
	m := metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.process_tree.other_v.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.process_tree.other_v.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory {
	m := metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.process_tree.python_rss.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.process_tree.python_rss.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory {
	m := metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.executor_metrics.process_tree.python_v.memory metric with initial data.
func (m *metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory) init() {
	m.data.SetName("databricks.spark.executor_metrics.process_tree.python_v.memory")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkExecutorMetricsProcessTreePythonVMemory(settings MetricSettings) metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory {
	m := metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkHiveExternalCatalogFileCacheHits struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.hive_external_catalog.file_cache.hits metric with initial data.
func (m *metricDatabricksSparkHiveExternalCatalogFileCacheHits) init() {
	m.data.SetName("databricks.spark.hive_external_catalog.file_cache.hits")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkHiveExternalCatalogFileCacheHits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkHiveExternalCatalogFileCacheHits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkHiveExternalCatalogFileCacheHits) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkHiveExternalCatalogFileCacheHits(settings MetricSettings) metricDatabricksSparkHiveExternalCatalogFileCacheHits {
	m := metricDatabricksSparkHiveExternalCatalogFileCacheHits{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkHiveExternalCatalogFilesDiscovered struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.hive_external_catalog.files_discovered metric with initial data.
func (m *metricDatabricksSparkHiveExternalCatalogFilesDiscovered) init() {
	m.data.SetName("databricks.spark.hive_external_catalog.files_discovered")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkHiveExternalCatalogFilesDiscovered) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkHiveExternalCatalogFilesDiscovered) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkHiveExternalCatalogFilesDiscovered) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkHiveExternalCatalogFilesDiscovered(settings MetricSettings) metricDatabricksSparkHiveExternalCatalogFilesDiscovered {
	m := metricDatabricksSparkHiveExternalCatalogFilesDiscovered{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkHiveExternalCatalogHiveClientCalls struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.hive_external_catalog.hive_client_calls metric with initial data.
func (m *metricDatabricksSparkHiveExternalCatalogHiveClientCalls) init() {
	m.data.SetName("databricks.spark.hive_external_catalog.hive_client_calls")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkHiveExternalCatalogHiveClientCalls) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkHiveExternalCatalogHiveClientCalls) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkHiveExternalCatalogHiveClientCalls) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkHiveExternalCatalogHiveClientCalls(settings MetricSettings) metricDatabricksSparkHiveExternalCatalogHiveClientCalls {
	m := metricDatabricksSparkHiveExternalCatalogHiveClientCalls{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.hive_external_catalog.parallel_listing_jobs.count metric with initial data.
func (m *metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount) init() {
	m.data.SetName("databricks.spark.hive_external_catalog.parallel_listing_jobs.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkHiveExternalCatalogParallelListingJobsCount(settings MetricSettings) metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount {
	m := metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkHiveExternalCatalogPartitionsFetched struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.hive_external_catalog.partitions_fetched metric with initial data.
func (m *metricDatabricksSparkHiveExternalCatalogPartitionsFetched) init() {
	m.data.SetName("databricks.spark.hive_external_catalog.partitions_fetched")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkHiveExternalCatalogPartitionsFetched) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkHiveExternalCatalogPartitionsFetched) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkHiveExternalCatalogPartitionsFetched) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkHiveExternalCatalogPartitionsFetched(settings MetricSettings) metricDatabricksSparkHiveExternalCatalogPartitionsFetched {
	m := metricDatabricksSparkHiveExternalCatalogPartitionsFetched{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkJobNumActiveStages struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.job.num_active_stages metric with initial data.
func (m *metricDatabricksSparkJobNumActiveStages) init() {
	m.data.SetName("databricks.spark.job.num_active_stages")
	m.data.SetDescription("n/a")
	m.data.SetUnit("{stages}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkJobNumActiveStages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkJobNumActiveStages) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkJobNumActiveStages) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkJobNumActiveStages(settings MetricSettings) metricDatabricksSparkJobNumActiveStages {
	m := metricDatabricksSparkJobNumActiveStages{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkJobNumActiveTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.job.num_active_tasks metric with initial data.
func (m *metricDatabricksSparkJobNumActiveTasks) init() {
	m.data.SetName("databricks.spark.job.num_active_tasks")
	m.data.SetDescription("n/a")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkJobNumActiveTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkJobNumActiveTasks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkJobNumActiveTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkJobNumActiveTasks(settings MetricSettings) metricDatabricksSparkJobNumActiveTasks {
	m := metricDatabricksSparkJobNumActiveTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkJobNumCompletedStages struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.job.num_completed_stages metric with initial data.
func (m *metricDatabricksSparkJobNumCompletedStages) init() {
	m.data.SetName("databricks.spark.job.num_completed_stages")
	m.data.SetDescription("n/a")
	m.data.SetUnit("{stages}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkJobNumCompletedStages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkJobNumCompletedStages) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkJobNumCompletedStages) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkJobNumCompletedStages(settings MetricSettings) metricDatabricksSparkJobNumCompletedStages {
	m := metricDatabricksSparkJobNumCompletedStages{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkJobNumCompletedTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.job.num_completed_tasks metric with initial data.
func (m *metricDatabricksSparkJobNumCompletedTasks) init() {
	m.data.SetName("databricks.spark.job.num_completed_tasks")
	m.data.SetDescription("n/a")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkJobNumCompletedTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkJobNumCompletedTasks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkJobNumCompletedTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkJobNumCompletedTasks(settings MetricSettings) metricDatabricksSparkJobNumCompletedTasks {
	m := metricDatabricksSparkJobNumCompletedTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkJobNumFailedStages struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.job.num_failed_stages metric with initial data.
func (m *metricDatabricksSparkJobNumFailedStages) init() {
	m.data.SetName("databricks.spark.job.num_failed_stages")
	m.data.SetDescription("n/a")
	m.data.SetUnit("{stages}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkJobNumFailedStages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkJobNumFailedStages) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkJobNumFailedStages) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkJobNumFailedStages(settings MetricSettings) metricDatabricksSparkJobNumFailedStages {
	m := metricDatabricksSparkJobNumFailedStages{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkJobNumFailedTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.job.num_failed_tasks metric with initial data.
func (m *metricDatabricksSparkJobNumFailedTasks) init() {
	m.data.SetName("databricks.spark.job.num_failed_tasks")
	m.data.SetDescription("n/a")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkJobNumFailedTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkJobNumFailedTasks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkJobNumFailedTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkJobNumFailedTasks(settings MetricSettings) metricDatabricksSparkJobNumFailedTasks {
	m := metricDatabricksSparkJobNumFailedTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkJobNumSkippedStages struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.job.num_skipped_stages metric with initial data.
func (m *metricDatabricksSparkJobNumSkippedStages) init() {
	m.data.SetName("databricks.spark.job.num_skipped_stages")
	m.data.SetDescription("n/a")
	m.data.SetUnit("{stages}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkJobNumSkippedStages) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkJobNumSkippedStages) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkJobNumSkippedStages) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkJobNumSkippedStages(settings MetricSettings) metricDatabricksSparkJobNumSkippedStages {
	m := metricDatabricksSparkJobNumSkippedStages{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkJobNumSkippedTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.job.num_skipped_tasks metric with initial data.
func (m *metricDatabricksSparkJobNumSkippedTasks) init() {
	m.data.SetName("databricks.spark.job.num_skipped_tasks")
	m.data.SetDescription("n/a")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkJobNumSkippedTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkJobNumSkippedTasks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkJobNumSkippedTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkJobNumSkippedTasks(settings MetricSettings) metricDatabricksSparkJobNumSkippedTasks {
	m := metricDatabricksSparkJobNumSkippedTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkJobNumTasks struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.job.num_tasks metric with initial data.
func (m *metricDatabricksSparkJobNumTasks) init() {
	m.data.SetName("databricks.spark.job.num_tasks")
	m.data.SetDescription("n/a")
	m.data.SetUnit("{tasks}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkJobNumTasks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkJobNumTasks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkJobNumTasks) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkJobNumTasks(settings MetricSettings) metricDatabricksSparkJobNumTasks {
	m := metricDatabricksSparkJobNumTasks{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkJvmCPUTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.jvm.cpu.time metric with initial data.
func (m *metricDatabricksSparkJvmCPUTime) init() {
	m.data.SetName("databricks.spark.jvm.cpu.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkJvmCPUTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkJvmCPUTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkJvmCPUTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkJvmCPUTime(settings MetricSettings) metricDatabricksSparkJvmCPUTime {
	m := metricDatabricksSparkJvmCPUTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkLiveListenerBusEventsPostedCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.live_listener_bus.events_posted.count metric with initial data.
func (m *metricDatabricksSparkLiveListenerBusEventsPostedCount) init() {
	m.data.SetName("databricks.spark.live_listener_bus.events_posted.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkLiveListenerBusEventsPostedCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkLiveListenerBusEventsPostedCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkLiveListenerBusEventsPostedCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkLiveListenerBusEventsPostedCount(settings MetricSettings) metricDatabricksSparkLiveListenerBusEventsPostedCount {
	m := metricDatabricksSparkLiveListenerBusEventsPostedCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.live_listener_bus.queue.app_status.dropped_events.count metric with initial data.
func (m *metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount) init() {
	m.data.SetName("databricks.spark.live_listener_bus.queue.app_status.dropped_events.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount(settings MetricSettings) metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount {
	m := metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkLiveListenerBusQueueAppstatusSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.live_listener_bus.queue.appstatus.size metric with initial data.
func (m *metricDatabricksSparkLiveListenerBusQueueAppstatusSize) init() {
	m.data.SetName("databricks.spark.live_listener_bus.queue.appstatus.size")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkLiveListenerBusQueueAppstatusSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkLiveListenerBusQueueAppstatusSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkLiveListenerBusQueueAppstatusSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkLiveListenerBusQueueAppstatusSize(settings MetricSettings) metricDatabricksSparkLiveListenerBusQueueAppstatusSize {
	m := metricDatabricksSparkLiveListenerBusQueueAppstatusSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.live_listener_bus.queue.executor_management.dropped_events.count metric with initial data.
func (m *metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount) init() {
	m.data.SetName("databricks.spark.live_listener_bus.queue.executor_management.dropped_events.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount(settings MetricSettings) metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount {
	m := metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.live_listener_bus.queue.executormanagement.size metric with initial data.
func (m *metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize) init() {
	m.data.SetName("databricks.spark.live_listener_bus.queue.executormanagement.size")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkLiveListenerBusQueueExecutormanagementSize(settings MetricSettings) metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize {
	m := metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.live_listener_bus.queue.shared.dropped_events.count metric with initial data.
func (m *metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount) init() {
	m.data.SetName("databricks.spark.live_listener_bus.queue.shared.dropped_events.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount(settings MetricSettings) metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount {
	m := metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkLiveListenerBusQueueSharedSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.live_listener_bus.queue.shared.size metric with initial data.
func (m *metricDatabricksSparkLiveListenerBusQueueSharedSize) init() {
	m.data.SetName("databricks.spark.live_listener_bus.queue.shared.size")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkLiveListenerBusQueueSharedSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkLiveListenerBusQueueSharedSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkLiveListenerBusQueueSharedSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkLiveListenerBusQueueSharedSize(settings MetricSettings) metricDatabricksSparkLiveListenerBusQueueSharedSize {
	m := metricDatabricksSparkLiveListenerBusQueueSharedSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.live_listener_bus.queue.streams.dropped_events.count metric with initial data.
func (m *metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount) init() {
	m.data.SetName("databricks.spark.live_listener_bus.queue.streams.dropped_events.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount(settings MetricSettings) metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount {
	m := metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkLiveListenerBusQueueStreamsSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.live_listener_bus.queue.streams.size metric with initial data.
func (m *metricDatabricksSparkLiveListenerBusQueueStreamsSize) init() {
	m.data.SetName("databricks.spark.live_listener_bus.queue.streams.size")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkLiveListenerBusQueueStreamsSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkLiveListenerBusQueueStreamsSize) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkLiveListenerBusQueueStreamsSize) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkLiveListenerBusQueueStreamsSize(settings MetricSettings) metricDatabricksSparkLiveListenerBusQueueStreamsSize {
	m := metricDatabricksSparkLiveListenerBusQueueStreamsSize{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.spark_sql_operation_manager.hive_operations.count metric with initial data.
func (m *metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount) init() {
	m.data.SetName("databricks.spark.spark_sql_operation_manager.hive_operations.count")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutStr("pipeline.id", pipelineIDAttributeValue)
	dp.Attributes().PutStr("pipeline.name", pipelineNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount(settings MetricSettings) metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount {
	m := metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkStageDiskBytesSpilled struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.stage.disk_bytes_spilled metric with initial data.
func (m *metricDatabricksSparkStageDiskBytesSpilled) init() {
	m.data.SetName("databricks.spark.stage.disk_bytes_spilled")
	m.data.SetDescription("n/a")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkStageDiskBytesSpilled) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkStageDiskBytesSpilled) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkStageDiskBytesSpilled) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkStageDiskBytesSpilled(settings MetricSettings) metricDatabricksSparkStageDiskBytesSpilled {
	m := metricDatabricksSparkStageDiskBytesSpilled{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkStageExecutorRunTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.stage.executor_run_time metric with initial data.
func (m *metricDatabricksSparkStageExecutorRunTime) init() {
	m.data.SetName("databricks.spark.stage.executor_run_time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkStageExecutorRunTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkStageExecutorRunTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkStageExecutorRunTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkStageExecutorRunTime(settings MetricSettings) metricDatabricksSparkStageExecutorRunTime {
	m := metricDatabricksSparkStageExecutorRunTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkStageInputBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.stage.input_bytes metric with initial data.
func (m *metricDatabricksSparkStageInputBytes) init() {
	m.data.SetName("databricks.spark.stage.input_bytes")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkStageInputBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkStageInputBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkStageInputBytes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkStageInputBytes(settings MetricSettings) metricDatabricksSparkStageInputBytes {
	m := metricDatabricksSparkStageInputBytes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkStageInputRecords struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.stage.input_records metric with initial data.
func (m *metricDatabricksSparkStageInputRecords) init() {
	m.data.SetName("databricks.spark.stage.input_records")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkStageInputRecords) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkStageInputRecords) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkStageInputRecords) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkStageInputRecords(settings MetricSettings) metricDatabricksSparkStageInputRecords {
	m := metricDatabricksSparkStageInputRecords{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkStageMemoryBytesSpilled struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.stage.memory_bytes_spilled metric with initial data.
func (m *metricDatabricksSparkStageMemoryBytesSpilled) init() {
	m.data.SetName("databricks.spark.stage.memory_bytes_spilled")
	m.data.SetDescription("n/a")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkStageMemoryBytesSpilled) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkStageMemoryBytesSpilled) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkStageMemoryBytesSpilled) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkStageMemoryBytesSpilled(settings MetricSettings) metricDatabricksSparkStageMemoryBytesSpilled {
	m := metricDatabricksSparkStageMemoryBytesSpilled{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkStageOutputBytes struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.stage.output_bytes metric with initial data.
func (m *metricDatabricksSparkStageOutputBytes) init() {
	m.data.SetName("databricks.spark.stage.output_bytes")
	m.data.SetDescription("n/a")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkStageOutputBytes) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkStageOutputBytes) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkStageOutputBytes) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkStageOutputBytes(settings MetricSettings) metricDatabricksSparkStageOutputBytes {
	m := metricDatabricksSparkStageOutputBytes{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkStageOutputRecords struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.stage.output_records metric with initial data.
func (m *metricDatabricksSparkStageOutputRecords) init() {
	m.data.SetName("databricks.spark.stage.output_records")
	m.data.SetDescription("n/a")
	m.data.SetUnit("")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkStageOutputRecords) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
	dp.Attributes().PutInt("spark.job.id", sparkJobIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkStageOutputRecords) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkStageOutputRecords) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkStageOutputRecords(settings MetricSettings) metricDatabricksSparkStageOutputRecords {
	m := metricDatabricksSparkStageOutputRecords{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerDagSchedulerMessageProcessingTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.dag_scheduler.message_processing.time metric with initial data.
func (m *metricDatabricksSparkTimerDagSchedulerMessageProcessingTime) init() {
	m.data.SetName("databricks.spark.timer.dag_scheduler.message_processing.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerDagSchedulerMessageProcessingTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerDagSchedulerMessageProcessingTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerDagSchedulerMessageProcessingTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerDagSchedulerMessageProcessingTime(settings MetricSettings) metricDatabricksSparkTimerDagSchedulerMessageProcessingTime {
	m := metricDatabricksSparkTimerDagSchedulerMessageProcessingTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.streaming.query_listener_bus.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.streaming.query_listener_bus.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.ui.sql_app_status_listener.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.ui.sql_app_status_listener.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.hive.thriftserver.ui.hive_thrift_server2listener.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.hive.thriftserver.ui.hive_thrift_server2listener.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.spark_session.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.spark_session.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.util.execution_listener_bus.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.util.execution_listener_bus.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.status.app_status_listener.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.status.app_status_listener.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.util.profiler_env.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.util.profiler_env.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.data_plane_event_listener.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.data_plane_event_listener.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.dbc_event_logging_listener.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.dbc_event_logging_listener.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.databricks.photon.photon_cleanup_listener.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.databricks.photon.photon_cleanup_listener.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.executor_time_logging_listener.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.executor_time_logging_listener.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.usage_logging_listener.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.usage_logging_listener.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.advice.advisor_listener.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.advice.advisor_listener.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.debugger.query_watchdog_listener.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.debugger.query_watchdog_listener.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.execution.ui.io_cache_listener.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.execution.ui.io_cache_listener.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.io.caching.repeated_reads_estimator.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.io.caching.repeated_reads_estimator.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime {
	m := metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.queue.app_status.listener_processing.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.queue.app_status.listener_processing.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime {
	m := metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.queue.executor_management.listener_processing.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.queue.executor_management.listener_processing.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime {
	m := metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.queue.shared.listener_processing.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.queue.shared.listener_processing.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime {
	m := metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.spark.timer.live_listener_bus.queue.streams.listener_processing.time metric with initial data.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime) init() {
	m.data.SetName("databricks.spark.timer.live_listener_bus.queue.streams.listener_processing.time")
	m.data.SetDescription("n/a")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityDelta)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("cluster.id", clusterIDAttributeValue)
	dp.Attributes().PutStr("spark.app.id", sparkAppIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime(settings MetricSettings) metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime {
	m := metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksTasksRunDuration struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.tasks.run.duration metric with initial data.
func (m *metricDatabricksTasksRunDuration) init() {
	m.data.SetName("databricks.tasks.run.duration")
	m.data.SetDescription("The execution duration in milliseconds per completed task")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksTasksRunDuration) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jobIDAttributeValue int64, taskIDAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutInt("job.id", jobIDAttributeValue)
	dp.Attributes().PutStr("task.id", taskIDAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksTasksRunDuration) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksTasksRunDuration) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksTasksRunDuration(settings MetricSettings) metricDatabricksTasksRunDuration {
	m := metricDatabricksTasksRunDuration{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricDatabricksTasksScheduleStatus struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills databricks.tasks.schedule.status metric with initial data.
func (m *metricDatabricksTasksScheduleStatus) init() {
	m.data.SetName("databricks.tasks.schedule.status")
	m.data.SetDescription("A snapshot of the pause/run status per task taken at each scrape")
	m.data.SetUnit("{status}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricDatabricksTasksScheduleStatus) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, jobIDAttributeValue int64, taskIDAttributeValue string, taskTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutInt("job.id", jobIDAttributeValue)
	dp.Attributes().PutStr("task.id", taskIDAttributeValue)
	dp.Attributes().PutStr("task.type", taskTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricDatabricksTasksScheduleStatus) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricDatabricksTasksScheduleStatus) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricDatabricksTasksScheduleStatus(settings MetricSettings) metricDatabricksTasksScheduleStatus {
	m := metricDatabricksTasksScheduleStatus{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user settings.
type MetricsBuilder struct {
	metricsBuffer                                                                                                            pmetric.Metrics
	buildInfo                                                                                                                component.BuildInfo
	metricDatabricksJobsActiveTotal                                                                                          metricDatabricksJobsActiveTotal
	metricDatabricksJobsRunDuration                                                                                          metricDatabricksJobsRunDuration
	metricDatabricksJobsScheduleStatus                                                                                       metricDatabricksJobsScheduleStatus
	metricDatabricksJobsTotal                                                                                                metricDatabricksJobsTotal
	metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed                                                                     metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed
	metricDatabricksSparkBlockManagerMemoryMax                                                                               metricDatabricksSparkBlockManagerMemoryMax
	metricDatabricksSparkBlockManagerMemoryOffHeapMax                                                                        metricDatabricksSparkBlockManagerMemoryOffHeapMax
	metricDatabricksSparkBlockManagerMemoryOffHeapUsed                                                                       metricDatabricksSparkBlockManagerMemoryOffHeapUsed
	metricDatabricksSparkBlockManagerMemoryOnHeapMax                                                                         metricDatabricksSparkBlockManagerMemoryOnHeapMax
	metricDatabricksSparkBlockManagerMemoryOnHeapUsed                                                                        metricDatabricksSparkBlockManagerMemoryOnHeapUsed
	metricDatabricksSparkBlockManagerMemoryRemaining                                                                         metricDatabricksSparkBlockManagerMemoryRemaining
	metricDatabricksSparkBlockManagerMemoryRemainingOffHeap                                                                  metricDatabricksSparkBlockManagerMemoryRemainingOffHeap
	metricDatabricksSparkBlockManagerMemoryRemainingOnHeap                                                                   metricDatabricksSparkBlockManagerMemoryRemainingOnHeap
	metricDatabricksSparkBlockManagerMemoryUsed                                                                              metricDatabricksSparkBlockManagerMemoryUsed
	metricDatabricksSparkCodeGeneratorCompilationTime                                                                        metricDatabricksSparkCodeGeneratorCompilationTime
	metricDatabricksSparkCodeGeneratorGeneratedClassSize                                                                     metricDatabricksSparkCodeGeneratorGeneratedClassSize
	metricDatabricksSparkCodeGeneratorGeneratedMethodSize                                                                    metricDatabricksSparkCodeGeneratorGeneratedMethodSize
	metricDatabricksSparkCodeGeneratorSourcecodeSize                                                                         metricDatabricksSparkCodeGeneratorSourcecodeSize
	metricDatabricksSparkDagSchedulerJobsActive                                                                              metricDatabricksSparkDagSchedulerJobsActive
	metricDatabricksSparkDagSchedulerJobsAll                                                                                 metricDatabricksSparkDagSchedulerJobsAll
	metricDatabricksSparkDagSchedulerStagesFailed                                                                            metricDatabricksSparkDagSchedulerStagesFailed
	metricDatabricksSparkDagSchedulerStagesRunning                                                                           metricDatabricksSparkDagSchedulerStagesRunning
	metricDatabricksSparkDagSchedulerStagesWaiting                                                                           metricDatabricksSparkDagSchedulerStagesWaiting
	metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount                                                            metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount
	metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered                                                       metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered
	metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount                                                         metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount
	metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted                                                         metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted
	metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors                                                           metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors
	metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount                                                         metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount
	metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors                                                        metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors
	metricDatabricksSparkDatabricksDirectoryCommitMarkersRead                                                                metricDatabricksSparkDatabricksDirectoryCommitMarkersRead
	metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount                                                          metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount
	metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered                                                   metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered
	metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound                                                        metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound
	metricDatabricksSparkDatabricksDirectoryCommitVacuumCount                                                                metricDatabricksSparkDatabricksDirectoryCommitVacuumCount
	metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors                                                               metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors
	metricDatabricksSparkDatabricksPreemptionChecksCount                                                                     metricDatabricksSparkDatabricksPreemptionChecksCount
	metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount                                                           metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount
	metricDatabricksSparkDatabricksPreemptionPoolstarvationTime                                                              metricDatabricksSparkDatabricksPreemptionPoolstarvationTime
	metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime                                                           metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime
	metricDatabricksSparkDatabricksPreemptionTaskWastedTime                                                                  metricDatabricksSparkDatabricksPreemptionTaskWastedTime
	metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount                                                             metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount
	metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools                                                            metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools
	metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools                                                  metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools
	metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools                                                    metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools
	metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime                                           metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime
	metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools                                                 metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools
	metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned                                        metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned
	metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned                                          metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned
	metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount              metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount
	metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount                              metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount
	metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime                                   metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime
	metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount                                    metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount
	metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount                                          metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount
	metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount                                          metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount
	metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved                                           metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved
	metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools                                                    metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools
	metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished                                               metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished
	metricDatabricksSparkExecutorDiskUsed                                                                                    metricDatabricksSparkExecutorDiskUsed
	metricDatabricksSparkExecutorMaxMemory                                                                                   metricDatabricksSparkExecutorMaxMemory
	metricDatabricksSparkExecutorMemoryUsed                                                                                  metricDatabricksSparkExecutorMemoryUsed
	metricDatabricksSparkExecutorTotalInputBytes                                                                             metricDatabricksSparkExecutorTotalInputBytes
	metricDatabricksSparkExecutorTotalShuffleRead                                                                            metricDatabricksSparkExecutorTotalShuffleRead
	metricDatabricksSparkExecutorTotalShuffleWrite                                                                           metricDatabricksSparkExecutorTotalShuffleWrite
	metricDatabricksSparkExecutorMetricsDirectPoolMemory                                                                     metricDatabricksSparkExecutorMetricsDirectPoolMemory
	metricDatabricksSparkExecutorMetricsJvmHeapMemory                                                                        metricDatabricksSparkExecutorMetricsJvmHeapMemory
	metricDatabricksSparkExecutorMetricsJvmOffHeapMemory                                                                     metricDatabricksSparkExecutorMetricsJvmOffHeapMemory
	metricDatabricksSparkExecutorMetricsMajorGcCount                                                                         metricDatabricksSparkExecutorMetricsMajorGcCount
	metricDatabricksSparkExecutorMetricsMajorGcTime                                                                          metricDatabricksSparkExecutorMetricsMajorGcTime
	metricDatabricksSparkExecutorMetricsMappedPoolMemory                                                                     metricDatabricksSparkExecutorMetricsMappedPoolMemory
	metricDatabricksSparkExecutorMetricsMinorGcCount                                                                         metricDatabricksSparkExecutorMetricsMinorGcCount
	metricDatabricksSparkExecutorMetricsMinorGcTime                                                                          metricDatabricksSparkExecutorMetricsMinorGcTime
	metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory                                                               metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory
	metricDatabricksSparkExecutorMetricsOffHeapStorageMemory                                                                 metricDatabricksSparkExecutorMetricsOffHeapStorageMemory
	metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory                                                                 metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory
	metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory                                                                metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory
	metricDatabricksSparkExecutorMetricsOnHeapStorageMemory                                                                  metricDatabricksSparkExecutorMetricsOnHeapStorageMemory
	metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory                                                                  metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory
	metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory                                                              metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory
	metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory                                                                metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory
	metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory                                                            metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory
	metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory                                                              metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory
	metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory                                                           metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory
	metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory                                                             metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory
	metricDatabricksSparkHiveExternalCatalogFileCacheHits                                                                    metricDatabricksSparkHiveExternalCatalogFileCacheHits
	metricDatabricksSparkHiveExternalCatalogFilesDiscovered                                                                  metricDatabricksSparkHiveExternalCatalogFilesDiscovered
	metricDatabricksSparkHiveExternalCatalogHiveClientCalls                                                                  metricDatabricksSparkHiveExternalCatalogHiveClientCalls
	metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount                                                         metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount
	metricDatabricksSparkHiveExternalCatalogPartitionsFetched                                                                metricDatabricksSparkHiveExternalCatalogPartitionsFetched
	metricDatabricksSparkJobNumActiveStages                                                                                  metricDatabricksSparkJobNumActiveStages
	metricDatabricksSparkJobNumActiveTasks                                                                                   metricDatabricksSparkJobNumActiveTasks
	metricDatabricksSparkJobNumCompletedStages                                                                               metricDatabricksSparkJobNumCompletedStages
	metricDatabricksSparkJobNumCompletedTasks                                                                                metricDatabricksSparkJobNumCompletedTasks
	metricDatabricksSparkJobNumFailedStages                                                                                  metricDatabricksSparkJobNumFailedStages
	metricDatabricksSparkJobNumFailedTasks                                                                                   metricDatabricksSparkJobNumFailedTasks
	metricDatabricksSparkJobNumSkippedStages                                                                                 metricDatabricksSparkJobNumSkippedStages
	metricDatabricksSparkJobNumSkippedTasks                                                                                  metricDatabricksSparkJobNumSkippedTasks
	metricDatabricksSparkJobNumTasks                                                                                         metricDatabricksSparkJobNumTasks
	metricDatabricksSparkJvmCPUTime                                                                                          metricDatabricksSparkJvmCPUTime
	metricDatabricksSparkLiveListenerBusEventsPostedCount                                                                    metricDatabricksSparkLiveListenerBusEventsPostedCount
	metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount                                                     metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount
	metricDatabricksSparkLiveListenerBusQueueAppstatusSize                                                                   metricDatabricksSparkLiveListenerBusQueueAppstatusSize
	metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount                                            metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount
	metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize                                                          metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize
	metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount                                                        metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount
	metricDatabricksSparkLiveListenerBusQueueSharedSize                                                                      metricDatabricksSparkLiveListenerBusQueueSharedSize
	metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount                                                       metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount
	metricDatabricksSparkLiveListenerBusQueueStreamsSize                                                                     metricDatabricksSparkLiveListenerBusQueueStreamsSize
	metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount                                                         metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount
	metricDatabricksSparkStageDiskBytesSpilled                                                                               metricDatabricksSparkStageDiskBytesSpilled
	metricDatabricksSparkStageExecutorRunTime                                                                                metricDatabricksSparkStageExecutorRunTime
	metricDatabricksSparkStageInputBytes                                                                                     metricDatabricksSparkStageInputBytes
	metricDatabricksSparkStageInputRecords                                                                                   metricDatabricksSparkStageInputRecords
	metricDatabricksSparkStageMemoryBytesSpilled                                                                             metricDatabricksSparkStageMemoryBytesSpilled
	metricDatabricksSparkStageOutputBytes                                                                                    metricDatabricksSparkStageOutputBytes
	metricDatabricksSparkStageOutputRecords                                                                                  metricDatabricksSparkStageOutputRecords
	metricDatabricksSparkTimerDagSchedulerMessageProcessingTime                                                              metricDatabricksSparkTimerDagSchedulerMessageProcessingTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime          metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime                                   metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime             metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime                                metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime                    metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime                        metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime                                metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime       metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime      metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime                     metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime            metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime                   metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime                        metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime                metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime                   metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime
	metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime              metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime
	metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime                                            metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime
	metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime                                   metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime
	metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime                                               metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime
	metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime                                              metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime
	metricDatabricksTasksRunDuration                                                                                         metricDatabricksTasksRunDuration
	metricDatabricksTasksScheduleStatus                                                                                      metricDatabricksTasksScheduleStatus
	startTime                                                                                                                pcommon.Timestamp
	metricsCapacity                                                                                                          int
	resourceCapacity                                                                                                         int
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func NewMetricsBuilder(settings MetricsSettings, buildInfo component.BuildInfo, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		startTime:                          pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                      pmetric.NewMetrics(),
		buildInfo:                          buildInfo,
		metricDatabricksJobsActiveTotal:    newMetricDatabricksJobsActiveTotal(settings.DatabricksJobsActiveTotal),
		metricDatabricksJobsRunDuration:    newMetricDatabricksJobsRunDuration(settings.DatabricksJobsRunDuration),
		metricDatabricksJobsScheduleStatus: newMetricDatabricksJobsScheduleStatus(settings.DatabricksJobsScheduleStatus),
		metricDatabricksJobsTotal:          newMetricDatabricksJobsTotal(settings.DatabricksJobsTotal),
		metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed:                                                                     newMetricDatabricksSparkBlockManagerMemoryDiskSpaceUsed(settings.DatabricksSparkBlockManagerMemoryDiskSpaceUsed),
		metricDatabricksSparkBlockManagerMemoryMax:                                                                               newMetricDatabricksSparkBlockManagerMemoryMax(settings.DatabricksSparkBlockManagerMemoryMax),
		metricDatabricksSparkBlockManagerMemoryOffHeapMax:                                                                        newMetricDatabricksSparkBlockManagerMemoryOffHeapMax(settings.DatabricksSparkBlockManagerMemoryOffHeapMax),
		metricDatabricksSparkBlockManagerMemoryOffHeapUsed:                                                                       newMetricDatabricksSparkBlockManagerMemoryOffHeapUsed(settings.DatabricksSparkBlockManagerMemoryOffHeapUsed),
		metricDatabricksSparkBlockManagerMemoryOnHeapMax:                                                                         newMetricDatabricksSparkBlockManagerMemoryOnHeapMax(settings.DatabricksSparkBlockManagerMemoryOnHeapMax),
		metricDatabricksSparkBlockManagerMemoryOnHeapUsed:                                                                        newMetricDatabricksSparkBlockManagerMemoryOnHeapUsed(settings.DatabricksSparkBlockManagerMemoryOnHeapUsed),
		metricDatabricksSparkBlockManagerMemoryRemaining:                                                                         newMetricDatabricksSparkBlockManagerMemoryRemaining(settings.DatabricksSparkBlockManagerMemoryRemaining),
		metricDatabricksSparkBlockManagerMemoryRemainingOffHeap:                                                                  newMetricDatabricksSparkBlockManagerMemoryRemainingOffHeap(settings.DatabricksSparkBlockManagerMemoryRemainingOffHeap),
		metricDatabricksSparkBlockManagerMemoryRemainingOnHeap:                                                                   newMetricDatabricksSparkBlockManagerMemoryRemainingOnHeap(settings.DatabricksSparkBlockManagerMemoryRemainingOnHeap),
		metricDatabricksSparkBlockManagerMemoryUsed:                                                                              newMetricDatabricksSparkBlockManagerMemoryUsed(settings.DatabricksSparkBlockManagerMemoryUsed),
		metricDatabricksSparkCodeGeneratorCompilationTime:                                                                        newMetricDatabricksSparkCodeGeneratorCompilationTime(settings.DatabricksSparkCodeGeneratorCompilationTime),
		metricDatabricksSparkCodeGeneratorGeneratedClassSize:                                                                     newMetricDatabricksSparkCodeGeneratorGeneratedClassSize(settings.DatabricksSparkCodeGeneratorGeneratedClassSize),
		metricDatabricksSparkCodeGeneratorGeneratedMethodSize:                                                                    newMetricDatabricksSparkCodeGeneratorGeneratedMethodSize(settings.DatabricksSparkCodeGeneratorGeneratedMethodSize),
		metricDatabricksSparkCodeGeneratorSourcecodeSize:                                                                         newMetricDatabricksSparkCodeGeneratorSourcecodeSize(settings.DatabricksSparkCodeGeneratorSourcecodeSize),
		metricDatabricksSparkDagSchedulerJobsActive:                                                                              newMetricDatabricksSparkDagSchedulerJobsActive(settings.DatabricksSparkDagSchedulerJobsActive),
		metricDatabricksSparkDagSchedulerJobsAll:                                                                                 newMetricDatabricksSparkDagSchedulerJobsAll(settings.DatabricksSparkDagSchedulerJobsAll),
		metricDatabricksSparkDagSchedulerStagesFailed:                                                                            newMetricDatabricksSparkDagSchedulerStagesFailed(settings.DatabricksSparkDagSchedulerStagesFailed),
		metricDatabricksSparkDagSchedulerStagesRunning:                                                                           newMetricDatabricksSparkDagSchedulerStagesRunning(settings.DatabricksSparkDagSchedulerStagesRunning),
		metricDatabricksSparkDagSchedulerStagesWaiting:                                                                           newMetricDatabricksSparkDagSchedulerStagesWaiting(settings.DatabricksSparkDagSchedulerStagesWaiting),
		metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount:                                                            newMetricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount(settings.DatabricksSparkDatabricksDirectoryCommitAutoVacuumCount),
		metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered:                                                       newMetricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered(settings.DatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered),
		metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount:                                                         newMetricDatabricksSparkDatabricksDirectoryCommitFilterListingCount(settings.DatabricksSparkDatabricksDirectoryCommitFilterListingCount),
		metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted:                                                         newMetricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted(settings.DatabricksSparkDatabricksDirectoryCommitJobCommitCompleted),
		metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors:                                                           newMetricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors(settings.DatabricksSparkDatabricksDirectoryCommitMarkerReadErrors),
		metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount:                                                         newMetricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount(settings.DatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount),
		metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors:                                                        newMetricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors(settings.DatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors),
		metricDatabricksSparkDatabricksDirectoryCommitMarkersRead:                                                                newMetricDatabricksSparkDatabricksDirectoryCommitMarkersRead(settings.DatabricksSparkDatabricksDirectoryCommitMarkersRead),
		metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount:                                                          newMetricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount(settings.DatabricksSparkDatabricksDirectoryCommitRepeatedListCount),
		metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered:                                                   newMetricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered(settings.DatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered),
		metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound:                                                        newMetricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound(settings.DatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound),
		metricDatabricksSparkDatabricksDirectoryCommitVacuumCount:                                                                newMetricDatabricksSparkDatabricksDirectoryCommitVacuumCount(settings.DatabricksSparkDatabricksDirectoryCommitVacuumCount),
		metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors:                                                               newMetricDatabricksSparkDatabricksDirectoryCommitVacuumErrors(settings.DatabricksSparkDatabricksDirectoryCommitVacuumErrors),
		metricDatabricksSparkDatabricksPreemptionChecksCount:                                                                     newMetricDatabricksSparkDatabricksPreemptionChecksCount(settings.DatabricksSparkDatabricksPreemptionChecksCount),
		metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount:                                                           newMetricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount(settings.DatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount),
		metricDatabricksSparkDatabricksPreemptionPoolstarvationTime:                                                              newMetricDatabricksSparkDatabricksPreemptionPoolstarvationTime(settings.DatabricksSparkDatabricksPreemptionPoolstarvationTime),
		metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime:                                                           newMetricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime(settings.DatabricksSparkDatabricksPreemptionSchedulerOverheadTime),
		metricDatabricksSparkDatabricksPreemptionTaskWastedTime:                                                                  newMetricDatabricksSparkDatabricksPreemptionTaskWastedTime(settings.DatabricksSparkDatabricksPreemptionTaskWastedTime),
		metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount:                                                             newMetricDatabricksSparkDatabricksPreemptionTasksPreemptedCount(settings.DatabricksSparkDatabricksPreemptionTasksPreemptedCount),
		metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools:                                                            newMetricDatabricksSparkDatabricksTaskSchedulingLanesActivePools(settings.DatabricksSparkDatabricksTaskSchedulingLanesActivePools),
		metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools:                                                  newMetricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools(settings.DatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools),
		metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools:                                                    newMetricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools(settings.DatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools),
		metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime:                                           newMetricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime(settings.DatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime),
		metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools:                                                 newMetricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools(settings.DatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools),
		metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned:                                        newMetricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned(settings.DatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned),
		metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned:                                          newMetricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned(settings.DatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned),
		metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount:              newMetricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount(settings.DatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount),
		metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount:                              newMetricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount(settings.DatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount),
		metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime:                                   newMetricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime(settings.DatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime),
		metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount:                                    newMetricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount(settings.DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount),
		metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount:                                          newMetricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount(settings.DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount),
		metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount:                                          newMetricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount(settings.DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount),
		metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved:                                           newMetricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved(settings.DatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved),
		metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools:                                                    newMetricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools(settings.DatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools),
		metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished:                                               newMetricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished(settings.DatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished),
		metricDatabricksSparkExecutorDiskUsed:                                                                                    newMetricDatabricksSparkExecutorDiskUsed(settings.DatabricksSparkExecutorDiskUsed),
		metricDatabricksSparkExecutorMaxMemory:                                                                                   newMetricDatabricksSparkExecutorMaxMemory(settings.DatabricksSparkExecutorMaxMemory),
		metricDatabricksSparkExecutorMemoryUsed:                                                                                  newMetricDatabricksSparkExecutorMemoryUsed(settings.DatabricksSparkExecutorMemoryUsed),
		metricDatabricksSparkExecutorTotalInputBytes:                                                                             newMetricDatabricksSparkExecutorTotalInputBytes(settings.DatabricksSparkExecutorTotalInputBytes),
		metricDatabricksSparkExecutorTotalShuffleRead:                                                                            newMetricDatabricksSparkExecutorTotalShuffleRead(settings.DatabricksSparkExecutorTotalShuffleRead),
		metricDatabricksSparkExecutorTotalShuffleWrite:                                                                           newMetricDatabricksSparkExecutorTotalShuffleWrite(settings.DatabricksSparkExecutorTotalShuffleWrite),
		metricDatabricksSparkExecutorMetricsDirectPoolMemory:                                                                     newMetricDatabricksSparkExecutorMetricsDirectPoolMemory(settings.DatabricksSparkExecutorMetricsDirectPoolMemory),
		metricDatabricksSparkExecutorMetricsJvmHeapMemory:                                                                        newMetricDatabricksSparkExecutorMetricsJvmHeapMemory(settings.DatabricksSparkExecutorMetricsJvmHeapMemory),
		metricDatabricksSparkExecutorMetricsJvmOffHeapMemory:                                                                     newMetricDatabricksSparkExecutorMetricsJvmOffHeapMemory(settings.DatabricksSparkExecutorMetricsJvmOffHeapMemory),
		metricDatabricksSparkExecutorMetricsMajorGcCount:                                                                         newMetricDatabricksSparkExecutorMetricsMajorGcCount(settings.DatabricksSparkExecutorMetricsMajorGcCount),
		metricDatabricksSparkExecutorMetricsMajorGcTime:                                                                          newMetricDatabricksSparkExecutorMetricsMajorGcTime(settings.DatabricksSparkExecutorMetricsMajorGcTime),
		metricDatabricksSparkExecutorMetricsMappedPoolMemory:                                                                     newMetricDatabricksSparkExecutorMetricsMappedPoolMemory(settings.DatabricksSparkExecutorMetricsMappedPoolMemory),
		metricDatabricksSparkExecutorMetricsMinorGcCount:                                                                         newMetricDatabricksSparkExecutorMetricsMinorGcCount(settings.DatabricksSparkExecutorMetricsMinorGcCount),
		metricDatabricksSparkExecutorMetricsMinorGcTime:                                                                          newMetricDatabricksSparkExecutorMetricsMinorGcTime(settings.DatabricksSparkExecutorMetricsMinorGcTime),
		metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory:                                                               newMetricDatabricksSparkExecutorMetricsOffHeapExecutionMemory(settings.DatabricksSparkExecutorMetricsOffHeapExecutionMemory),
		metricDatabricksSparkExecutorMetricsOffHeapStorageMemory:                                                                 newMetricDatabricksSparkExecutorMetricsOffHeapStorageMemory(settings.DatabricksSparkExecutorMetricsOffHeapStorageMemory),
		metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory:                                                                 newMetricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory(settings.DatabricksSparkExecutorMetricsOffHeapUnifiedMemory),
		metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory:                                                                newMetricDatabricksSparkExecutorMetricsOnHeapExecutionMemory(settings.DatabricksSparkExecutorMetricsOnHeapExecutionMemory),
		metricDatabricksSparkExecutorMetricsOnHeapStorageMemory:                                                                  newMetricDatabricksSparkExecutorMetricsOnHeapStorageMemory(settings.DatabricksSparkExecutorMetricsOnHeapStorageMemory),
		metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory:                                                                  newMetricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory(settings.DatabricksSparkExecutorMetricsOnHeapUnifiedMemory),
		metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory:                                                              newMetricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory(settings.DatabricksSparkExecutorMetricsProcessTreeJvmRssMemory),
		metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory:                                                                newMetricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory(settings.DatabricksSparkExecutorMetricsProcessTreeJvmVMemory),
		metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory:                                                            newMetricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory(settings.DatabricksSparkExecutorMetricsProcessTreeOtherRssMemory),
		metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory:                                                              newMetricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory(settings.DatabricksSparkExecutorMetricsProcessTreeOtherVMemory),
		metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory:                                                           newMetricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory(settings.DatabricksSparkExecutorMetricsProcessTreePythonRssMemory),
		metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory:                                                             newMetricDatabricksSparkExecutorMetricsProcessTreePythonVMemory(settings.DatabricksSparkExecutorMetricsProcessTreePythonVMemory),
		metricDatabricksSparkHiveExternalCatalogFileCacheHits:                                                                    newMetricDatabricksSparkHiveExternalCatalogFileCacheHits(settings.DatabricksSparkHiveExternalCatalogFileCacheHits),
		metricDatabricksSparkHiveExternalCatalogFilesDiscovered:                                                                  newMetricDatabricksSparkHiveExternalCatalogFilesDiscovered(settings.DatabricksSparkHiveExternalCatalogFilesDiscovered),
		metricDatabricksSparkHiveExternalCatalogHiveClientCalls:                                                                  newMetricDatabricksSparkHiveExternalCatalogHiveClientCalls(settings.DatabricksSparkHiveExternalCatalogHiveClientCalls),
		metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount:                                                         newMetricDatabricksSparkHiveExternalCatalogParallelListingJobsCount(settings.DatabricksSparkHiveExternalCatalogParallelListingJobsCount),
		metricDatabricksSparkHiveExternalCatalogPartitionsFetched:                                                                newMetricDatabricksSparkHiveExternalCatalogPartitionsFetched(settings.DatabricksSparkHiveExternalCatalogPartitionsFetched),
		metricDatabricksSparkJobNumActiveStages:                                                                                  newMetricDatabricksSparkJobNumActiveStages(settings.DatabricksSparkJobNumActiveStages),
		metricDatabricksSparkJobNumActiveTasks:                                                                                   newMetricDatabricksSparkJobNumActiveTasks(settings.DatabricksSparkJobNumActiveTasks),
		metricDatabricksSparkJobNumCompletedStages:                                                                               newMetricDatabricksSparkJobNumCompletedStages(settings.DatabricksSparkJobNumCompletedStages),
		metricDatabricksSparkJobNumCompletedTasks:                                                                                newMetricDatabricksSparkJobNumCompletedTasks(settings.DatabricksSparkJobNumCompletedTasks),
		metricDatabricksSparkJobNumFailedStages:                                                                                  newMetricDatabricksSparkJobNumFailedStages(settings.DatabricksSparkJobNumFailedStages),
		metricDatabricksSparkJobNumFailedTasks:                                                                                   newMetricDatabricksSparkJobNumFailedTasks(settings.DatabricksSparkJobNumFailedTasks),
		metricDatabricksSparkJobNumSkippedStages:                                                                                 newMetricDatabricksSparkJobNumSkippedStages(settings.DatabricksSparkJobNumSkippedStages),
		metricDatabricksSparkJobNumSkippedTasks:                                                                                  newMetricDatabricksSparkJobNumSkippedTasks(settings.DatabricksSparkJobNumSkippedTasks),
		metricDatabricksSparkJobNumTasks:                                                                                         newMetricDatabricksSparkJobNumTasks(settings.DatabricksSparkJobNumTasks),
		metricDatabricksSparkJvmCPUTime:                                                                                          newMetricDatabricksSparkJvmCPUTime(settings.DatabricksSparkJvmCPUTime),
		metricDatabricksSparkLiveListenerBusEventsPostedCount:                                                                    newMetricDatabricksSparkLiveListenerBusEventsPostedCount(settings.DatabricksSparkLiveListenerBusEventsPostedCount),
		metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount:                                                     newMetricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount(settings.DatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount),
		metricDatabricksSparkLiveListenerBusQueueAppstatusSize:                                                                   newMetricDatabricksSparkLiveListenerBusQueueAppstatusSize(settings.DatabricksSparkLiveListenerBusQueueAppstatusSize),
		metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount:                                            newMetricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount(settings.DatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount),
		metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize:                                                          newMetricDatabricksSparkLiveListenerBusQueueExecutormanagementSize(settings.DatabricksSparkLiveListenerBusQueueExecutormanagementSize),
		metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount:                                                        newMetricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount(settings.DatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount),
		metricDatabricksSparkLiveListenerBusQueueSharedSize:                                                                      newMetricDatabricksSparkLiveListenerBusQueueSharedSize(settings.DatabricksSparkLiveListenerBusQueueSharedSize),
		metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount:                                                       newMetricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount(settings.DatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount),
		metricDatabricksSparkLiveListenerBusQueueStreamsSize:                                                                     newMetricDatabricksSparkLiveListenerBusQueueStreamsSize(settings.DatabricksSparkLiveListenerBusQueueStreamsSize),
		metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount:                                                         newMetricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount(settings.DatabricksSparkSparkSQLOperationManagerHiveOperationsCount),
		metricDatabricksSparkStageDiskBytesSpilled:                                                                               newMetricDatabricksSparkStageDiskBytesSpilled(settings.DatabricksSparkStageDiskBytesSpilled),
		metricDatabricksSparkStageExecutorRunTime:                                                                                newMetricDatabricksSparkStageExecutorRunTime(settings.DatabricksSparkStageExecutorRunTime),
		metricDatabricksSparkStageInputBytes:                                                                                     newMetricDatabricksSparkStageInputBytes(settings.DatabricksSparkStageInputBytes),
		metricDatabricksSparkStageInputRecords:                                                                                   newMetricDatabricksSparkStageInputRecords(settings.DatabricksSparkStageInputRecords),
		metricDatabricksSparkStageMemoryBytesSpilled:                                                                             newMetricDatabricksSparkStageMemoryBytesSpilled(settings.DatabricksSparkStageMemoryBytesSpilled),
		metricDatabricksSparkStageOutputBytes:                                                                                    newMetricDatabricksSparkStageOutputBytes(settings.DatabricksSparkStageOutputBytes),
		metricDatabricksSparkStageOutputRecords:                                                                                  newMetricDatabricksSparkStageOutputRecords(settings.DatabricksSparkStageOutputRecords),
		metricDatabricksSparkTimerDagSchedulerMessageProcessingTime:                                                              newMetricDatabricksSparkTimerDagSchedulerMessageProcessingTime(settings.DatabricksSparkTimerDagSchedulerMessageProcessingTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime:          newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime:                                   newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime:             newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime: newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime:                                newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime:                    newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime:                        newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime:                                newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime:       newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime:      newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime:                     newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime:            newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime:                   newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime:                        newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime:                newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime:                   newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime),
		metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime:              newMetricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime(settings.DatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime),
		metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime:                                            newMetricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime(settings.DatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime),
		metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime:                                   newMetricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime(settings.DatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime),
		metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime:                                               newMetricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime(settings.DatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime),
		metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime:                                              newMetricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime(settings.DatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime),
		metricDatabricksTasksRunDuration:                                                                                         newMetricDatabricksTasksRunDuration(settings.DatabricksTasksRunDuration),
		metricDatabricksTasksScheduleStatus:                                                                                      newMetricDatabricksTasksScheduleStatus(settings.DatabricksTasksScheduleStatus),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
	if mb.resourceCapacity < rm.Resource().Attributes().Len() {
		mb.resourceCapacity = rm.Resource().Attributes().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption func(pmetric.ResourceMetrics)

// WithDatabricksInstanceName sets provided value as "databricks.instance.name" attribute for current resource.
func WithDatabricksInstanceName(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().PutStr("databricks.instance.name", val)
	}
}

// WithSparkAppID sets provided value as "spark.app.id" attribute for current resource.
func WithSparkAppID(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().PutStr("spark.app.id", val)
	}
}

// WithSparkClusterID sets provided value as "spark.cluster.id" attribute for current resource.
func WithSparkClusterID(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().PutStr("spark.cluster.id", val)
	}
}

// WithSparkClusterName sets provided value as "spark.cluster.name" attribute for current resource.
func WithSparkClusterName(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().PutStr("spark.cluster.name", val)
	}
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	}
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(rmo ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	rm.Resource().Attributes().EnsureCapacity(mb.resourceCapacity)
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName("otelcol/databricksreceiver")
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricDatabricksJobsActiveTotal.emit(ils.Metrics())
	mb.metricDatabricksJobsRunDuration.emit(ils.Metrics())
	mb.metricDatabricksJobsScheduleStatus.emit(ils.Metrics())
	mb.metricDatabricksJobsTotal.emit(ils.Metrics())
	mb.metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed.emit(ils.Metrics())
	mb.metricDatabricksSparkBlockManagerMemoryMax.emit(ils.Metrics())
	mb.metricDatabricksSparkBlockManagerMemoryOffHeapMax.emit(ils.Metrics())
	mb.metricDatabricksSparkBlockManagerMemoryOffHeapUsed.emit(ils.Metrics())
	mb.metricDatabricksSparkBlockManagerMemoryOnHeapMax.emit(ils.Metrics())
	mb.metricDatabricksSparkBlockManagerMemoryOnHeapUsed.emit(ils.Metrics())
	mb.metricDatabricksSparkBlockManagerMemoryRemaining.emit(ils.Metrics())
	mb.metricDatabricksSparkBlockManagerMemoryRemainingOffHeap.emit(ils.Metrics())
	mb.metricDatabricksSparkBlockManagerMemoryRemainingOnHeap.emit(ils.Metrics())
	mb.metricDatabricksSparkBlockManagerMemoryUsed.emit(ils.Metrics())
	mb.metricDatabricksSparkCodeGeneratorCompilationTime.emit(ils.Metrics())
	mb.metricDatabricksSparkCodeGeneratorGeneratedClassSize.emit(ils.Metrics())
	mb.metricDatabricksSparkCodeGeneratorGeneratedMethodSize.emit(ils.Metrics())
	mb.metricDatabricksSparkCodeGeneratorSourcecodeSize.emit(ils.Metrics())
	mb.metricDatabricksSparkDagSchedulerJobsActive.emit(ils.Metrics())
	mb.metricDatabricksSparkDagSchedulerJobsAll.emit(ils.Metrics())
	mb.metricDatabricksSparkDagSchedulerStagesFailed.emit(ils.Metrics())
	mb.metricDatabricksSparkDagSchedulerStagesRunning.emit(ils.Metrics())
	mb.metricDatabricksSparkDagSchedulerStagesWaiting.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitMarkersRead.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitVacuumCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksPreemptionChecksCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksPreemptionPoolstarvationTime.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksPreemptionTaskWastedTime.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools.emit(ils.Metrics())
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorDiskUsed.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMaxMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMemoryUsed.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorTotalInputBytes.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorTotalShuffleRead.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorTotalShuffleWrite.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsDirectPoolMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsJvmHeapMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsJvmOffHeapMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsMajorGcCount.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsMajorGcTime.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsMappedPoolMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsMinorGcCount.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsMinorGcTime.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsOffHeapStorageMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsOnHeapStorageMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory.emit(ils.Metrics())
	mb.metricDatabricksSparkHiveExternalCatalogFileCacheHits.emit(ils.Metrics())
	mb.metricDatabricksSparkHiveExternalCatalogFilesDiscovered.emit(ils.Metrics())
	mb.metricDatabricksSparkHiveExternalCatalogHiveClientCalls.emit(ils.Metrics())
	mb.metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount.emit(ils.Metrics())
	mb.metricDatabricksSparkHiveExternalCatalogPartitionsFetched.emit(ils.Metrics())
	mb.metricDatabricksSparkJobNumActiveStages.emit(ils.Metrics())
	mb.metricDatabricksSparkJobNumActiveTasks.emit(ils.Metrics())
	mb.metricDatabricksSparkJobNumCompletedStages.emit(ils.Metrics())
	mb.metricDatabricksSparkJobNumCompletedTasks.emit(ils.Metrics())
	mb.metricDatabricksSparkJobNumFailedStages.emit(ils.Metrics())
	mb.metricDatabricksSparkJobNumFailedTasks.emit(ils.Metrics())
	mb.metricDatabricksSparkJobNumSkippedStages.emit(ils.Metrics())
	mb.metricDatabricksSparkJobNumSkippedTasks.emit(ils.Metrics())
	mb.metricDatabricksSparkJobNumTasks.emit(ils.Metrics())
	mb.metricDatabricksSparkJvmCPUTime.emit(ils.Metrics())
	mb.metricDatabricksSparkLiveListenerBusEventsPostedCount.emit(ils.Metrics())
	mb.metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount.emit(ils.Metrics())
	mb.metricDatabricksSparkLiveListenerBusQueueAppstatusSize.emit(ils.Metrics())
	mb.metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount.emit(ils.Metrics())
	mb.metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize.emit(ils.Metrics())
	mb.metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount.emit(ils.Metrics())
	mb.metricDatabricksSparkLiveListenerBusQueueSharedSize.emit(ils.Metrics())
	mb.metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount.emit(ils.Metrics())
	mb.metricDatabricksSparkLiveListenerBusQueueStreamsSize.emit(ils.Metrics())
	mb.metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount.emit(ils.Metrics())
	mb.metricDatabricksSparkStageDiskBytesSpilled.emit(ils.Metrics())
	mb.metricDatabricksSparkStageExecutorRunTime.emit(ils.Metrics())
	mb.metricDatabricksSparkStageInputBytes.emit(ils.Metrics())
	mb.metricDatabricksSparkStageInputRecords.emit(ils.Metrics())
	mb.metricDatabricksSparkStageMemoryBytesSpilled.emit(ils.Metrics())
	mb.metricDatabricksSparkStageOutputBytes.emit(ils.Metrics())
	mb.metricDatabricksSparkStageOutputRecords.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerDagSchedulerMessageProcessingTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime.emit(ils.Metrics())
	mb.metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime.emit(ils.Metrics())
	mb.metricDatabricksTasksRunDuration.emit(ils.Metrics())
	mb.metricDatabricksTasksScheduleStatus.emit(ils.Metrics())
	for _, op := range rmo {
		op(rm)
	}
	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user settings, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(rmo ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(rmo...)
	metrics := pmetric.NewMetrics()
	mb.metricsBuffer.MoveTo(metrics)
	return metrics
}

// RecordDatabricksJobsActiveTotalDataPoint adds a data point to databricks.jobs.active.total metric.
func (mb *MetricsBuilder) RecordDatabricksJobsActiveTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricDatabricksJobsActiveTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordDatabricksJobsRunDurationDataPoint adds a data point to databricks.jobs.run.duration metric.
func (mb *MetricsBuilder) RecordDatabricksJobsRunDurationDataPoint(ts pcommon.Timestamp, val int64, jobIDAttributeValue int64) {
	mb.metricDatabricksJobsRunDuration.recordDataPoint(mb.startTime, ts, val, jobIDAttributeValue)
}

// RecordDatabricksJobsScheduleStatusDataPoint adds a data point to databricks.jobs.schedule.status metric.
func (mb *MetricsBuilder) RecordDatabricksJobsScheduleStatusDataPoint(ts pcommon.Timestamp, val int64, jobIDAttributeValue int64) {
	mb.metricDatabricksJobsScheduleStatus.recordDataPoint(mb.startTime, ts, val, jobIDAttributeValue)
}

// RecordDatabricksJobsTotalDataPoint adds a data point to databricks.jobs.total metric.
func (mb *MetricsBuilder) RecordDatabricksJobsTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricDatabricksJobsTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordDatabricksSparkBlockManagerMemoryDiskSpaceUsedDataPoint adds a data point to databricks.spark.block_manager.memory.disk_space.used metric.
func (mb *MetricsBuilder) RecordDatabricksSparkBlockManagerMemoryDiskSpaceUsedDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkBlockManagerMemoryDiskSpaceUsed.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkBlockManagerMemoryMaxDataPoint adds a data point to databricks.spark.block_manager.memory.max metric.
func (mb *MetricsBuilder) RecordDatabricksSparkBlockManagerMemoryMaxDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkBlockManagerMemoryMax.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkBlockManagerMemoryOffHeapMaxDataPoint adds a data point to databricks.spark.block_manager.memory.off_heap.max metric.
func (mb *MetricsBuilder) RecordDatabricksSparkBlockManagerMemoryOffHeapMaxDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkBlockManagerMemoryOffHeapMax.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkBlockManagerMemoryOffHeapUsedDataPoint adds a data point to databricks.spark.block_manager.memory.off_heap.used metric.
func (mb *MetricsBuilder) RecordDatabricksSparkBlockManagerMemoryOffHeapUsedDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkBlockManagerMemoryOffHeapUsed.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkBlockManagerMemoryOnHeapMaxDataPoint adds a data point to databricks.spark.block_manager.memory.on_heap.max metric.
func (mb *MetricsBuilder) RecordDatabricksSparkBlockManagerMemoryOnHeapMaxDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkBlockManagerMemoryOnHeapMax.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkBlockManagerMemoryOnHeapUsedDataPoint adds a data point to databricks.spark.block_manager.memory.on_heap.used metric.
func (mb *MetricsBuilder) RecordDatabricksSparkBlockManagerMemoryOnHeapUsedDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkBlockManagerMemoryOnHeapUsed.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkBlockManagerMemoryRemainingDataPoint adds a data point to databricks.spark.block_manager.memory.remaining metric.
func (mb *MetricsBuilder) RecordDatabricksSparkBlockManagerMemoryRemainingDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkBlockManagerMemoryRemaining.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkBlockManagerMemoryRemainingOffHeapDataPoint adds a data point to databricks.spark.block_manager.memory.remaining.off_heap metric.
func (mb *MetricsBuilder) RecordDatabricksSparkBlockManagerMemoryRemainingOffHeapDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkBlockManagerMemoryRemainingOffHeap.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkBlockManagerMemoryRemainingOnHeapDataPoint adds a data point to databricks.spark.block_manager.memory.remaining.on_heap metric.
func (mb *MetricsBuilder) RecordDatabricksSparkBlockManagerMemoryRemainingOnHeapDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkBlockManagerMemoryRemainingOnHeap.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkBlockManagerMemoryUsedDataPoint adds a data point to databricks.spark.block_manager.memory.used metric.
func (mb *MetricsBuilder) RecordDatabricksSparkBlockManagerMemoryUsedDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkBlockManagerMemoryUsed.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkCodeGeneratorCompilationTimeDataPoint adds a data point to databricks.spark.code_generator.compilation.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkCodeGeneratorCompilationTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkCodeGeneratorCompilationTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkCodeGeneratorGeneratedClassSizeDataPoint adds a data point to databricks.spark.code_generator.generated_class_size metric.
func (mb *MetricsBuilder) RecordDatabricksSparkCodeGeneratorGeneratedClassSizeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkCodeGeneratorGeneratedClassSize.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkCodeGeneratorGeneratedMethodSizeDataPoint adds a data point to databricks.spark.code_generator.generated_method_size metric.
func (mb *MetricsBuilder) RecordDatabricksSparkCodeGeneratorGeneratedMethodSizeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkCodeGeneratorGeneratedMethodSize.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkCodeGeneratorSourcecodeSizeDataPoint adds a data point to databricks.spark.code_generator.sourcecode_size metric.
func (mb *MetricsBuilder) RecordDatabricksSparkCodeGeneratorSourcecodeSizeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkCodeGeneratorSourcecodeSize.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDagSchedulerJobsActiveDataPoint adds a data point to databricks.spark.dag_scheduler.jobs.active metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDagSchedulerJobsActiveDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDagSchedulerJobsActive.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDagSchedulerJobsAllDataPoint adds a data point to databricks.spark.dag_scheduler.jobs.all metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDagSchedulerJobsAllDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDagSchedulerJobsAll.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDagSchedulerStagesFailedDataPoint adds a data point to databricks.spark.dag_scheduler.stages.failed metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDagSchedulerStagesFailedDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDagSchedulerStagesFailed.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDagSchedulerStagesRunningDataPoint adds a data point to databricks.spark.dag_scheduler.stages.running metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDagSchedulerStagesRunningDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDagSchedulerStagesRunning.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDagSchedulerStagesWaitingDataPoint adds a data point to databricks.spark.dag_scheduler.stages.waiting metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDagSchedulerStagesWaitingDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDagSchedulerStagesWaiting.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitAutoVacuumCountDataPoint adds a data point to databricks.spark.databricks.directory_commit.auto_vacuum.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitAutoVacuumCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitAutoVacuumCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitDeletedFilesFilteredDataPoint adds a data point to databricks.spark.databricks.directory_commit.deleted_files_filtered metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitDeletedFilesFilteredDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitDeletedFilesFiltered.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitFilterListingCountDataPoint adds a data point to databricks.spark.databricks.directory_commit.filter_listing.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitFilterListingCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitFilterListingCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitJobCommitCompletedDataPoint adds a data point to databricks.spark.databricks.directory_commit.job_commit_completed metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitJobCommitCompletedDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitJobCommitCompleted.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitMarkerReadErrorsDataPoint adds a data point to databricks.spark.databricks.directory_commit.marker_read.errors metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitMarkerReadErrorsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitMarkerReadErrors.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCountDataPoint adds a data point to databricks.spark.databricks.directory_commit.marker_refresh.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrorsDataPoint adds a data point to databricks.spark.databricks.directory_commit.marker_refresh.errors metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrorsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrors.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitMarkersReadDataPoint adds a data point to databricks.spark.databricks.directory_commit.markers.read metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitMarkersReadDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitMarkersRead.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitRepeatedListCountDataPoint adds a data point to databricks.spark.databricks.directory_commit.repeated_list.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitRepeatedListCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitRepeatedListCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFilteredDataPoint adds a data point to databricks.spark.databricks.directory_commit.uncommitted_files.filtered metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFilteredDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFiltered.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFoundDataPoint adds a data point to databricks.spark.databricks.directory_commit.untracked_files.found metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFoundDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFound.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitVacuumCountDataPoint adds a data point to databricks.spark.databricks.directory_commit.vacuum.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitVacuumCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitVacuumCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksDirectoryCommitVacuumErrorsDataPoint adds a data point to databricks.spark.databricks.directory_commit.vacuum.errors metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksDirectoryCommitVacuumErrorsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksDirectoryCommitVacuumErrors.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksPreemptionChecksCountDataPoint adds a data point to databricks.spark.databricks.preemption.checks.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksPreemptionChecksCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksPreemptionChecksCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCountDataPoint adds a data point to databricks.spark.databricks.preemption.pools_autoexpired.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksPreemptionPoolstarvationTimeDataPoint adds a data point to databricks.spark.databricks.preemption.poolstarvation.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksPreemptionPoolstarvationTimeDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksPreemptionPoolstarvationTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksPreemptionSchedulerOverheadTimeDataPoint adds a data point to databricks.spark.databricks.preemption.scheduler_overhead.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksPreemptionSchedulerOverheadTimeDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksPreemptionSchedulerOverheadTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksPreemptionTaskWastedTimeDataPoint adds a data point to databricks.spark.databricks.preemption.task_wasted.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksPreemptionTaskWastedTimeDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksPreemptionTaskWastedTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksPreemptionTasksPreemptedCountDataPoint adds a data point to databricks.spark.databricks.preemption.tasks_preempted.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksPreemptionTasksPreemptedCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksPreemptionTasksPreemptedCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesActivePoolsDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.active_pools metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesActivePoolsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesActivePools.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePoolsDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.bypass_lane_active_pools metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePoolsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePools.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePoolsDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.fast_lane_active_pools metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePoolsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePools.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTimeDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.finished_queries_total_task.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTimeDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPoolsDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.marked_pools metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPoolsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPools.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleanedDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.two_phase_pools_cleaned metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleanedDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleaned.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleanedDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.zombie_pools_cleaned metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleanedDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleaned.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCountDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_successful_preemption_iterations.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCountDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_tasks_preempted.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTimeDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_wasted_task.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTimeDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCountDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.slot_reservation.gradual_decrease.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCountDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_drop.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCountDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_jump.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReservedDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.slot_reservation.slots_reserved metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReservedDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReserved.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePoolsDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.slow_lane_active_pools metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePoolsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePools.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinishedDataPoint adds a data point to databricks.spark.databricks.task_scheduling_lanes.totalquerygroupsfinished metric.
func (mb *MetricsBuilder) RecordDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinishedDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinished.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorDiskUsedDataPoint adds a data point to databricks.spark.executor.disk_used metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorDiskUsedDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	mb.metricDatabricksSparkExecutorDiskUsed.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkExecutorIDAttributeValue)
}

// RecordDatabricksSparkExecutorMaxMemoryDataPoint adds a data point to databricks.spark.executor.max_memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMaxMemoryDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	mb.metricDatabricksSparkExecutorMaxMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkExecutorIDAttributeValue)
}

// RecordDatabricksSparkExecutorMemoryUsedDataPoint adds a data point to databricks.spark.executor.memory_used metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMemoryUsedDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	mb.metricDatabricksSparkExecutorMemoryUsed.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkExecutorIDAttributeValue)
}

// RecordDatabricksSparkExecutorTotalInputBytesDataPoint adds a data point to databricks.spark.executor.total_input_bytes metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorTotalInputBytesDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	mb.metricDatabricksSparkExecutorTotalInputBytes.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkExecutorIDAttributeValue)
}

// RecordDatabricksSparkExecutorTotalShuffleReadDataPoint adds a data point to databricks.spark.executor.total_shuffle_read metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorTotalShuffleReadDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	mb.metricDatabricksSparkExecutorTotalShuffleRead.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkExecutorIDAttributeValue)
}

// RecordDatabricksSparkExecutorTotalShuffleWriteDataPoint adds a data point to databricks.spark.executor.total_shuffle_write metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorTotalShuffleWriteDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkExecutorIDAttributeValue string) {
	mb.metricDatabricksSparkExecutorTotalShuffleWrite.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkExecutorIDAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsDirectPoolMemoryDataPoint adds a data point to databricks.spark.executor_metrics.direct_pool.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsDirectPoolMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsDirectPoolMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsJvmHeapMemoryDataPoint adds a data point to databricks.spark.executor_metrics.jvm.heap.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsJvmHeapMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsJvmHeapMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsJvmOffHeapMemoryDataPoint adds a data point to databricks.spark.executor_metrics.jvm.off_heap.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsJvmOffHeapMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsJvmOffHeapMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsMajorGcCountDataPoint adds a data point to databricks.spark.executor_metrics.major_gc.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsMajorGcCountDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsMajorGcCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsMajorGcTimeDataPoint adds a data point to databricks.spark.executor_metrics.major_gc.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsMajorGcTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsMajorGcTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsMappedPoolMemoryDataPoint adds a data point to databricks.spark.executor_metrics.mapped_pool.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsMappedPoolMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsMappedPoolMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsMinorGcCountDataPoint adds a data point to databricks.spark.executor_metrics.minor_gc.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsMinorGcCountDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsMinorGcCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsMinorGcTimeDataPoint adds a data point to databricks.spark.executor_metrics.minor_gc.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsMinorGcTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsMinorGcTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsOffHeapExecutionMemoryDataPoint adds a data point to databricks.spark.executor_metrics.off_heap.execution.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsOffHeapExecutionMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsOffHeapExecutionMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsOffHeapStorageMemoryDataPoint adds a data point to databricks.spark.executor_metrics.off_heap.storage.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsOffHeapStorageMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsOffHeapStorageMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsOffHeapUnifiedMemoryDataPoint adds a data point to databricks.spark.executor_metrics.off_heap.unified.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsOffHeapUnifiedMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsOffHeapUnifiedMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsOnHeapExecutionMemoryDataPoint adds a data point to databricks.spark.executor_metrics.on_heap.execution.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsOnHeapExecutionMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsOnHeapExecutionMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsOnHeapStorageMemoryDataPoint adds a data point to databricks.spark.executor_metrics.on_heap.storage.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsOnHeapStorageMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsOnHeapStorageMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsOnHeapUnifiedMemoryDataPoint adds a data point to databricks.spark.executor_metrics.on_heap.unified.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsOnHeapUnifiedMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsOnHeapUnifiedMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsProcessTreeJvmRssMemoryDataPoint adds a data point to databricks.spark.executor_metrics.process_tree.jvm_rss.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsProcessTreeJvmRssMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsProcessTreeJvmRssMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsProcessTreeJvmVMemoryDataPoint adds a data point to databricks.spark.executor_metrics.process_tree.jvm_v.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsProcessTreeJvmVMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsProcessTreeJvmVMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsProcessTreeOtherRssMemoryDataPoint adds a data point to databricks.spark.executor_metrics.process_tree.other_rss.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsProcessTreeOtherRssMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsProcessTreeOtherRssMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsProcessTreeOtherVMemoryDataPoint adds a data point to databricks.spark.executor_metrics.process_tree.other_v.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsProcessTreeOtherVMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsProcessTreeOtherVMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsProcessTreePythonRssMemoryDataPoint adds a data point to databricks.spark.executor_metrics.process_tree.python_rss.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsProcessTreePythonRssMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsProcessTreePythonRssMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkExecutorMetricsProcessTreePythonVMemoryDataPoint adds a data point to databricks.spark.executor_metrics.process_tree.python_v.memory metric.
func (mb *MetricsBuilder) RecordDatabricksSparkExecutorMetricsProcessTreePythonVMemoryDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkExecutorMetricsProcessTreePythonVMemory.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkHiveExternalCatalogFileCacheHitsDataPoint adds a data point to databricks.spark.hive_external_catalog.file_cache.hits metric.
func (mb *MetricsBuilder) RecordDatabricksSparkHiveExternalCatalogFileCacheHitsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkHiveExternalCatalogFileCacheHits.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkHiveExternalCatalogFilesDiscoveredDataPoint adds a data point to databricks.spark.hive_external_catalog.files_discovered metric.
func (mb *MetricsBuilder) RecordDatabricksSparkHiveExternalCatalogFilesDiscoveredDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkHiveExternalCatalogFilesDiscovered.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkHiveExternalCatalogHiveClientCallsDataPoint adds a data point to databricks.spark.hive_external_catalog.hive_client_calls metric.
func (mb *MetricsBuilder) RecordDatabricksSparkHiveExternalCatalogHiveClientCallsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkHiveExternalCatalogHiveClientCalls.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkHiveExternalCatalogParallelListingJobsCountDataPoint adds a data point to databricks.spark.hive_external_catalog.parallel_listing_jobs.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkHiveExternalCatalogParallelListingJobsCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkHiveExternalCatalogParallelListingJobsCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkHiveExternalCatalogPartitionsFetchedDataPoint adds a data point to databricks.spark.hive_external_catalog.partitions_fetched metric.
func (mb *MetricsBuilder) RecordDatabricksSparkHiveExternalCatalogPartitionsFetchedDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkHiveExternalCatalogPartitionsFetched.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkJobNumActiveStagesDataPoint adds a data point to databricks.spark.job.num_active_stages metric.
func (mb *MetricsBuilder) RecordDatabricksSparkJobNumActiveStagesDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkJobNumActiveStages.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkJobNumActiveTasksDataPoint adds a data point to databricks.spark.job.num_active_tasks metric.
func (mb *MetricsBuilder) RecordDatabricksSparkJobNumActiveTasksDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkJobNumActiveTasks.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkJobNumCompletedStagesDataPoint adds a data point to databricks.spark.job.num_completed_stages metric.
func (mb *MetricsBuilder) RecordDatabricksSparkJobNumCompletedStagesDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkJobNumCompletedStages.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkJobNumCompletedTasksDataPoint adds a data point to databricks.spark.job.num_completed_tasks metric.
func (mb *MetricsBuilder) RecordDatabricksSparkJobNumCompletedTasksDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkJobNumCompletedTasks.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkJobNumFailedStagesDataPoint adds a data point to databricks.spark.job.num_failed_stages metric.
func (mb *MetricsBuilder) RecordDatabricksSparkJobNumFailedStagesDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkJobNumFailedStages.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkJobNumFailedTasksDataPoint adds a data point to databricks.spark.job.num_failed_tasks metric.
func (mb *MetricsBuilder) RecordDatabricksSparkJobNumFailedTasksDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkJobNumFailedTasks.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkJobNumSkippedStagesDataPoint adds a data point to databricks.spark.job.num_skipped_stages metric.
func (mb *MetricsBuilder) RecordDatabricksSparkJobNumSkippedStagesDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkJobNumSkippedStages.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkJobNumSkippedTasksDataPoint adds a data point to databricks.spark.job.num_skipped_tasks metric.
func (mb *MetricsBuilder) RecordDatabricksSparkJobNumSkippedTasksDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkJobNumSkippedTasks.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkJobNumTasksDataPoint adds a data point to databricks.spark.job.num_tasks metric.
func (mb *MetricsBuilder) RecordDatabricksSparkJobNumTasksDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkJobNumTasks.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkJvmCPUTimeDataPoint adds a data point to databricks.spark.jvm.cpu.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkJvmCPUTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkJvmCPUTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkLiveListenerBusEventsPostedCountDataPoint adds a data point to databricks.spark.live_listener_bus.events_posted.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkLiveListenerBusEventsPostedCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkLiveListenerBusEventsPostedCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCountDataPoint adds a data point to databricks.spark.live_listener_bus.queue.app_status.dropped_events.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkLiveListenerBusQueueAppstatusSizeDataPoint adds a data point to databricks.spark.live_listener_bus.queue.appstatus.size metric.
func (mb *MetricsBuilder) RecordDatabricksSparkLiveListenerBusQueueAppstatusSizeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkLiveListenerBusQueueAppstatusSize.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCountDataPoint adds a data point to databricks.spark.live_listener_bus.queue.executor_management.dropped_events.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkLiveListenerBusQueueExecutormanagementSizeDataPoint adds a data point to databricks.spark.live_listener_bus.queue.executormanagement.size metric.
func (mb *MetricsBuilder) RecordDatabricksSparkLiveListenerBusQueueExecutormanagementSizeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkLiveListenerBusQueueExecutormanagementSize.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCountDataPoint adds a data point to databricks.spark.live_listener_bus.queue.shared.dropped_events.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkLiveListenerBusQueueSharedSizeDataPoint adds a data point to databricks.spark.live_listener_bus.queue.shared.size metric.
func (mb *MetricsBuilder) RecordDatabricksSparkLiveListenerBusQueueSharedSizeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkLiveListenerBusQueueSharedSize.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCountDataPoint adds a data point to databricks.spark.live_listener_bus.queue.streams.dropped_events.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCountDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkLiveListenerBusQueueStreamsSizeDataPoint adds a data point to databricks.spark.live_listener_bus.queue.streams.size metric.
func (mb *MetricsBuilder) RecordDatabricksSparkLiveListenerBusQueueStreamsSizeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkLiveListenerBusQueueStreamsSize.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkSparkSQLOperationManagerHiveOperationsCountDataPoint adds a data point to databricks.spark.spark_sql_operation_manager.hive_operations.count metric.
func (mb *MetricsBuilder) RecordDatabricksSparkSparkSQLOperationManagerHiveOperationsCountDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, pipelineIDAttributeValue string, pipelineNameAttributeValue string) {
	mb.metricDatabricksSparkSparkSQLOperationManagerHiveOperationsCount.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, pipelineIDAttributeValue, pipelineNameAttributeValue)
}

// RecordDatabricksSparkStageDiskBytesSpilledDataPoint adds a data point to databricks.spark.stage.disk_bytes_spilled metric.
func (mb *MetricsBuilder) RecordDatabricksSparkStageDiskBytesSpilledDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkStageDiskBytesSpilled.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkStageExecutorRunTimeDataPoint adds a data point to databricks.spark.stage.executor_run_time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkStageExecutorRunTimeDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkStageExecutorRunTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkStageInputBytesDataPoint adds a data point to databricks.spark.stage.input_bytes metric.
func (mb *MetricsBuilder) RecordDatabricksSparkStageInputBytesDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkStageInputBytes.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkStageInputRecordsDataPoint adds a data point to databricks.spark.stage.input_records metric.
func (mb *MetricsBuilder) RecordDatabricksSparkStageInputRecordsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkStageInputRecords.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkStageMemoryBytesSpilledDataPoint adds a data point to databricks.spark.stage.memory_bytes_spilled metric.
func (mb *MetricsBuilder) RecordDatabricksSparkStageMemoryBytesSpilledDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkStageMemoryBytesSpilled.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkStageOutputBytesDataPoint adds a data point to databricks.spark.stage.output_bytes metric.
func (mb *MetricsBuilder) RecordDatabricksSparkStageOutputBytesDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkStageOutputBytes.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkStageOutputRecordsDataPoint adds a data point to databricks.spark.stage.output_records metric.
func (mb *MetricsBuilder) RecordDatabricksSparkStageOutputRecordsDataPoint(ts pcommon.Timestamp, val int64, clusterIDAttributeValue string, sparkAppIDAttributeValue string, sparkJobIDAttributeValue int64) {
	mb.metricDatabricksSparkStageOutputRecords.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue, sparkJobIDAttributeValue)
}

// RecordDatabricksSparkTimerDagSchedulerMessageProcessingTimeDataPoint adds a data point to databricks.spark.timer.dag_scheduler.message_processing.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerDagSchedulerMessageProcessingTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerDagSchedulerMessageProcessingTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.streaming.query_listener_bus.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.ui.sql_app_status_listener.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.hive.thriftserver.ui.hive_thrift_server2listener.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.spark_session.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.util.execution_listener_bus.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.status.app_status_listener.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.util.profiler_env.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.data_plane_event_listener.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.dbc_event_logging_listener.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.databricks.photon.photon_cleanup_listener.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.executor_time_logging_listener.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.usage_logging_listener.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.advice.advisor_listener.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.debugger.query_watchdog_listener.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.execution.ui.io_cache_listener.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.io.caching.repeated_reads_estimator.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.queue.app_status.listener_processing.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.queue.executor_management.listener_processing.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.queue.shared.listener_processing.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTimeDataPoint adds a data point to databricks.spark.timer.live_listener_bus.queue.streams.listener_processing.time metric.
func (mb *MetricsBuilder) RecordDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTimeDataPoint(ts pcommon.Timestamp, val float64, clusterIDAttributeValue string, sparkAppIDAttributeValue string) {
	mb.metricDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTime.recordDataPoint(mb.startTime, ts, val, clusterIDAttributeValue, sparkAppIDAttributeValue)
}

// RecordDatabricksTasksRunDurationDataPoint adds a data point to databricks.tasks.run.duration metric.
func (mb *MetricsBuilder) RecordDatabricksTasksRunDurationDataPoint(ts pcommon.Timestamp, val int64, jobIDAttributeValue int64, taskIDAttributeValue string) {
	mb.metricDatabricksTasksRunDuration.recordDataPoint(mb.startTime, ts, val, jobIDAttributeValue, taskIDAttributeValue)
}

// RecordDatabricksTasksScheduleStatusDataPoint adds a data point to databricks.tasks.schedule.status metric.
func (mb *MetricsBuilder) RecordDatabricksTasksScheduleStatusDataPoint(ts pcommon.Timestamp, val int64, jobIDAttributeValue int64, taskIDAttributeValue string, taskTypeAttributeValue AttributeTaskType) {
	mb.metricDatabricksTasksScheduleStatus.recordDataPoint(mb.startTime, ts, val, jobIDAttributeValue, taskIDAttributeValue, taskTypeAttributeValue.String())
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}
