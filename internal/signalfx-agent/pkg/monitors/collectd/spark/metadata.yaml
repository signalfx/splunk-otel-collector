monitors:
- dimensions:
    spark_process:
      description: Either master or worker to differentiate master- and worker- specific metrics like master.apps and worker.coresFree

    cluster:
      description: set to value corresponding to key `cluster` in configuration file
  doc: |
    This integration collects metrics about a Spark cluster using the [collectd Spark Python
    plugin](https://github.com/signalfx/collectd-spark). That plugin collects
    metrics from Spark cluster and instances by hitting endpoints specified in
    Spark's [Monitoring and Instrumentation
    documentation](https://spark.apache.org/docs/latest/monitoring.html) under
    `REST API` and `Metrics`.

    The following cluster modes are supported only through HTTP endpoints:
    - Standalone
    - Mesos
    - Hadoop YARN

    You must specify distinct monitor configurations and discovery rules for
    master and worker processes.  For the master configuration, set `isMaster`
    to true.

    When running Spark on Apache Hadoop / YARN, this integration is only capable
    of reporting application metrics from the master node.  Use the
    collectd/hadoop monitor to report on the health of the cluster.

    <!--- SETUP --->
    ### Example config:

    An example configuration for monitoring applications on YARN
    ```yaml
    monitors:
      - type: collectd/spark
        host: 000.000.000.000
        port: 8088
        clusterType: Yarn
        isMaster: true
        collectApplicationMetrics: true
    ```
  metrics:
    counter.HiveExternalCatalog.hiveClientCalls:
      description: Total number of client calls sent to Hive for query processing
      default: false
      type: counter
    counter.HiveExternalCatalog.fileCacheHits:
      description: Total number of file level cache hits occurred
      default: false
      type: counter
    counter.HiveExternalCatalog.filesDiscovered:
      description: Total number of files discovered
      default: false
      type: counter
    counter.HiveExternalCatalog.parallelListingJobCount:
      description: Total number of Hive-specific jobs running in parallel
      default: false
      type: counter
    counter.HiveExternalCatalog.partitionsFetched:
      description: Total number of partitions fetched
      default: false
      type: counter
    counter.spark.driver.completed_tasks:
      description: Total number of completed tasks in driver mapped to a particular
        application
      default: false
      type: counter
    counter.spark.driver.disk_used:
      description: Amount of disk used by driver mapped to a particular application
      default: true
      type: counter
    counter.spark.driver.failed_tasks:
      description: Total number of failed tasks in driver mapped to a particular application
      default: false
      type: counter
    counter.spark.driver.memory_used:
      description: Amount of memory used by driver mapped to a particular application
      default: true
      type: counter
    counter.spark.driver.total_duration:
      description: Fraction of time spent by driver mapped to a particular application
      default: false
      type: counter
    counter.spark.driver.total_input_bytes:
      description: Number of input bytes in driver mapped to a particular application
      default: true
      type: counter
    counter.spark.driver.total_shuffle_read:
      description: Size read during a shuffle in driver mapped to a particular application
      default: true
      type: counter
    counter.spark.driver.total_shuffle_write:
      description: Size written to during a shuffle in driver mapped to a particular
        application
      default: true
      type: counter
    counter.spark.driver.total_tasks:
      description: Total number of tasks in driver mapped to a particular application
      default: true
      type: counter
    counter.spark.executor.completed_tasks:
      description: Completed tasks across executors working for a particular application
      default: false
      type: counter
    counter.spark.executor.disk_used:
      description: Amount of disk used across executors working for a particular application
      default: true
      type: counter
    counter.spark.executor.failed_tasks:
      description: Failed tasks across executors working for a particular application
      default: false
      type: counter
    counter.spark.executor.memory_used:
      description: Amount of memory used across executors working for a particular
        application
      default: true
      type: counter
    counter.spark.executor.total_duration:
      description: Fraction of time spent across executors working for a particular
        application
      default: false
      type: counter
    counter.spark.executor.total_input_bytes:
      description: Number of input bytes across executors working for a particular
        application
      default: true
      type: counter
    counter.spark.executor.total_shuffle_read:
      description: Size read during a shuffle in a particular application's executors
      default: true
      type: counter
    counter.spark.executor.total_shuffle_write:
      description: Size written to during a shuffle in a particular application's
        executors
      default: true
      type: counter
    counter.spark.executor.total_tasks:
      description: Total tasks across executors working for a particular application
      default: false
      type: counter
    counter.spark.streaming.num_processed_records:
      description: Number of processed records in a streaming application
      default: true
      type: counter
    counter.spark.streaming.num_received_records:
      description: Number of received records in a streaming application
      default: true
      type: counter
    counter.spark.streaming.num_total_completed_batches:
      description: Number of batches completed in a streaming application
      default: true
      type: counter
    gauge.jvm.MarkSweepCompact.count:
      description: Garbage collection count
      default: false
      type: gauge
    gauge.jvm.MarkSweepCompact.time:
      description: Garbage collection time
      default: false
      type: gauge
    gauge.jvm.heap.committed:
      description: Amount of committed heap memory (in MB)
      default: true
      type: gauge
    gauge.jvm.heap.used:
      description: Amount of used heap memory (in MB)
      default: true
      type: gauge
    gauge.jvm.non-heap.committed:
      description: Amount of committed non-heap memory (in MB)
      default: true
      type: gauge
    gauge.jvm.non-heap.used:
      description: Amount of used non-heap memory (in MB)
      default: true
      type: gauge
    gauge.jvm.pools.Code-Cache.committed:
      description: Amount of memory committed for compilation and storage of native
        code
      default: false
      type: gauge
    gauge.jvm.pools.Code-Cache.used:
      description: Amount of memory used to compile and store native code
      default: false
      type: gauge
    gauge.jvm.pools.Compressed-Class-Space.committed:
      description: Amount of memory committed for compressing a class object
      default: false
      type: gauge
    gauge.jvm.pools.Compressed-Class-Space.used:
      description: Amount of memory used to compress a class object
      default: false
      type: gauge
    gauge.jvm.pools.Eden-Space.committed:
      description: Amount of memory committed for the initial allocation of objects
      default: false
      type: gauge
    gauge.jvm.pools.Eden-Space.used:
      description: Amount of memory used for the initial allocation of objects
      default: false
      type: gauge
    gauge.jvm.pools.Metaspace.committed:
      description: Amount of memory committed for storing classes and classloaders
      default: false
      type: gauge
    gauge.jvm.pools.Metaspace.used:
      description: Amount of memory used to store classes and classloaders
      default: false
      type: gauge
    gauge.jvm.pools.Survivor-Space.committed:
      description: Amount of memory committed specifically for objects that have survived
        GC of the Eden Space
      default: false
      type: gauge
    gauge.jvm.pools.Survivor-Space.used:
      description: Amount of memory used for objects that have survived GC of the
        Eden Space
      default: false
      type: gauge
    gauge.jvm.pools.Tenured-Gen.committed:
      description: Amount of memory committed to store objects that have lived in
        the survivor space for a given period of time
      default: false
      type: gauge
    gauge.jvm.pools.Tenured-Gen.used:
      description: Amount of memory used for objects that have lived in the survivor
        space for a given period of time
      default: false
      type: gauge
    gauge.jvm.total.committed:
      description: Amount of committed JVM memory (in MB)
      default: true
      type: gauge
    gauge.jvm.total.used:
      description: Amount of used JVM memory (in MB)
      default: true
      type: gauge
    gauge.master.aliveWorkers:
      description: Total functioning workers
      default: true
      type: gauge
    gauge.master.apps:
      description: Total number of active applications in the spark cluster
      default: true
      type: gauge
    gauge.master.waitingApps:
      description: Total number of waiting applications in the spark cluster
      default: true
      type: gauge
    gauge.master.workers:
      description: Total number of workers in spark cluster
      default: true
      type: gauge
    gauge.spark.driver.active_tasks:
      description: Total number of active tasks in driver mapped to a particular application
      default: false
      type: gauge
    gauge.spark.driver.max_memory:
      description: Maximum memory used by driver mapped to a particular application
      default: true
      type: gauge
    gauge.spark.driver.rdd_blocks:
      description: Number of RDD blocks in the driver mapped to a particular application
      default: false
      type: gauge
    gauge.spark.executor.active_tasks:
      description: Total number of active tasks across all executors working for a
        particular application
      default: false
      type: gauge
    gauge.spark.executor.count:
      description: Total number of executors performing for an active application
        in the spark cluster
      default: true
      type: gauge
    gauge.spark.executor.max_memory:
      description: Max memory across all executors working for a particular application
      default: true
      type: gauge
    gauge.spark.executor.rdd_blocks:
      description: Number of RDD blocks across all executors working for a particular
        application
      default: false
      type: gauge
    gauge.spark.job.num_active_stages:
      description: Total number of active stages for an active application in the
        spark cluster
      default: true
      type: gauge
    gauge.spark.job.num_active_tasks:
      description: Total number of active tasks for an active application in the spark
        cluster
      default: true
      type: gauge
    gauge.spark.job.num_completed_stages:
      description: Total number of completed stages for an active application in the
        spark cluster
      default: true
      type: gauge
    gauge.spark.job.num_completed_tasks:
      description: Total number of completed tasks for an active application in the
        spark cluster
      default: true
      type: gauge
    gauge.spark.job.num_failed_stages:
      description: Total number of failed stages for an active application in the
        spark cluster
      default: true
      type: gauge
    gauge.spark.job.num_failed_tasks:
      description: Total number of failed tasks for an active application in the spark
        cluster
      default: true
      type: gauge
    gauge.spark.job.num_skipped_stages:
      description: Total number of skipped stages for an active application in the
        spark cluster
      default: true
      type: gauge
    gauge.spark.job.num_skipped_tasks:
      description: Total number of skipped tasks for an active application in the
        spark cluster
      default: true
      type: gauge
    gauge.spark.job.num_tasks:
      description: Total number of tasks for an active application in the spark cluster
      default: true
      type: gauge
    gauge.spark.num_active_stages:
      description: Total number of active stages for an active application in the
        spark cluster
      default: true
      type: gauge
    gauge.spark.num_running_jobs:
      description: Total number of running jobs for an active application in the spark
        cluster
      default: true
      type: gauge
    gauge.spark.stage.disk_bytes_spilled:
      description: Actual size written to disk for an active application in the spark
        cluster
      default: true
      type: gauge
    gauge.spark.stage.executor_run_time:
      description: Fraction of time spent by (and averaged across) executors for a
        particular application
      default: true
      type: gauge
    gauge.spark.stage.input_bytes:
      description: Input size for a particular application
      default: true
      type: gauge
    gauge.spark.stage.input_records:
      description: Input records received for a particular application
      default: true
      type: gauge
    gauge.spark.stage.memory_bytes_spilled:
      description: Size spilled to disk from memory for an active application in the
        spark cluster
      default: true
      type: gauge
    gauge.spark.stage.output_bytes:
      description: Output size for a particular application
      default: true
      type: gauge
    gauge.spark.stage.output_records:
      description: Output records written to for a particular application
      default: true
      type: gauge
    gauge.spark.stage.shuffle_read_bytes:
      description: Read size during shuffle phase for a particular application
      default: false
      type: gauge
    gauge.spark.stage.shuffle_read_records:
      description: Number of records read during shuffle phase for a particular application
      default: false
      type: gauge
    gauge.spark.stage.shuffle_write_bytes:
      description: Size written during shuffle phase for a particular application
      default: false
      type: gauge
    gauge.spark.stage.shuffle_write_records:
      description: Number of records written to during shuffle phase for a particular
        application
      default: false
      type: gauge
    gauge.spark.streaming.avg_input_rate:
      description: Average input rate of records across retained batches in a streaming
        application
      default: true
      type: gauge
    gauge.spark.streaming.avg_processing_time:
      description: Average processing time in a streaming application
      default: true
      type: gauge
    gauge.spark.streaming.avg_scheduling_delay:
      description: Average scheduling delay in a streaming application
      default: true
      type: gauge
    gauge.spark.streaming.avg_total_delay:
      description: Average total delay in a streaming application
      default: true
      type: gauge
    gauge.spark.streaming.num_active_batches:
      description: Number of active batches in a streaming application
      default: true
      type: gauge
    gauge.spark.streaming.num_inactive_receivers:
      description: Number of inactive receivers in a streaming application
      default: true
      type: gauge
    gauge.worker.coresFree:
      description: Total cores free for a particular worker process
      default: true
      type: gauge
    gauge.worker.coresUsed:
      description: Total cores used by a particular worker process
      default: true
      type: gauge
    gauge.worker.executors:
      description: Total number of executors for a particular worker process
      default: true
      type: gauge
    gauge.worker.memFree_MB:
      description: Total memory free for a particular worker process
      default: true
      type: gauge
    gauge.worker.memUsed_MB:
      description: Memory used by a particular worker process
      default: true
      type: gauge
  monitorType: collectd/spark
  properties:
